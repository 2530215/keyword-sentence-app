from collections import Counter
import streamlit as st
from gensim.models import Word2Vec
import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import re
from konlpy.tag import Okt # KoNLPy Okt ì¶”ê°€

# --- Okt í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” ---
okt = Okt()

# --- ë¶ˆìš©ì–´ ë° ê¸°ë³¸ ì„¤ì • ---
# ë¶ˆìš©ì–´ ëª©ë¡ (ìƒê¸°ë¶€ ë‚´ìš©ì— ë§ì¶° ê³„ì† ì¶”ê°€/ìˆ˜ì •í•˜ì‹œëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤)
STOPWORDS = [
    # --- ê¸°ì¡´ì— ì‚¬ìš©í•˜ì‹œë˜ ëª©ë¡ ---
    'ìˆ˜', 'ê²ƒ', 'ë•Œ', 'ë“±', 'ì´', 'ê·¸', 'ì €', 'ë…„', 'ì›”', 'ì¼', 'ì¢€', 'ì¤‘', 'ìœ„í•´', 'ë°',
    'ê·¸ê²ƒ', 'ì´ê²ƒ', 'ì €ê²ƒ', 'ì—¬ê¸°', 'ì €ê¸°', 'ê±°ê¸°', 'ìì‹ ', 'ìì²´', 'ëŒ€í•œ', 'í†µí•´', 'ê´€ë ¨',
    'ì—¬ëŸ¬', 'ê°€ì§€', 'ë‹¤ë¥¸', 'ë¶€ë¶„', 'ê²½ìš°', 'ì •ë„', 'ì‚¬ì´', 'ë¬¸ì œ', 'ë‚´ìš©', 'ê²°ê³¼', 'ê³¼ì •',
    'ì‚¬ìš©', 'ìƒê°', 'ì§€ê¸ˆ', 'í˜„ì¬', 'ë‹¹ì‹œ', 'ë•Œë¬¸ì—', 'ë©´ì„œ', 'ë™ì•ˆ', 'ìœ„í•œ', 'ë”°ë¼',
    'ëŒ€í•´', 'í†µí•œ', 'ê´€ë ¨ëœ', 'ìˆìŒ', 'ì—†ìŒ', 'ê°™ìŒ', 'ì‚¬í•­', 'í™œë™', 'ëª¨ìŠµ', 'ë¶„ì•¼', # 'ëª¨ìŠµ' ì¤‘ë³µ ì œê±°ë¨
    'ëŠ¥ë ¥', # 'ëª¨ìŠµ'ì€ ìœ„ì—ì„œ ì´ë¯¸ ì²˜ë¦¬, 'ì—­ëŸ‰', 'ìì„¸', 'íƒœë„', 'ë…¸ë ¥', 'ë°”íƒ•', 'ì—­í• ', 'í•™ìŠµ', 'ì´í•´'ëŠ” ì•„ë˜ ëª©ë¡ê³¼ ì¤‘ë³µë  ìˆ˜ ìˆì–´ í™•ì¸
    'í•­ìƒ', 'ë§¤ìš°', 'ë‹¤ì†Œ', 'íŠ¹íˆ', 'ê°€ì¥', 'ë”ìš±', 'ì ê·¹ì ', 'êµ¬ì²´ì ', 'ë‹¤ì–‘í•œ', 'ê¾¸ì¤€íˆ',
    'ë›°ì–´ë‚¨', 'ìš°ìˆ˜í•¨', 'ë³´ì„', 'ë°œíœ˜í•¨', 'ì°¸ì—¬í•¨', 'íƒêµ¬í•¨', 'ë°œì „í•¨', 'í–¥ìƒë¨', 'í•¨ì–‘í•¨', # 'ì°¸ì—¬í•¨', 'íƒêµ¬í•¨', 'ë°œì „í•¨', 'í–¥ìƒë¨' ë“±ì€ ì•„ë˜ ëª©ë¡ê³¼ ìœ ì‚¬/ì¤‘ë³µ ê°€ëŠ¥
    'ë§Œë“¦', 'ë°œí‘œí•¨', 'ì œì‹œí•¨', 'ì œì¶œí•¨', 'ë°”', 'ì ', 'ì¸¡ë©´', 'ê³¼ì œ', 'ì¡°ì‚¬', 'ì£¼ì œ', # 'ê³¼ì œ', 'ì¡°ì‚¬', 'ì£¼ì œ', 'ë°œí‘œí•¨' ë“±ì€ ì•„ë˜ ëª©ë¡ê³¼ ì¤‘ë³µ ê°€ëŠ¥
    'ìë£Œ', 'ë°œí‘œ', 'í† ë¡ ', 'ë³´ê³ ì„œ', 'íƒêµ¬', 'ì—°êµ¬', 'í”„ë¡œì íŠ¸', 'ì‹¤í—˜', 'ìˆ˜ì—…', 'ì‹œê°„', # 'ë°œí‘œ', 'í† ë¡ ', 'ë³´ê³ ì„œ', 'íƒêµ¬', 'ì—°êµ¬', 'í”„ë¡œì íŠ¸', 'ì‹¤í—˜', 'ìˆ˜ì—…', 'ì‹œê°„' ë“±ì€ ì•„ë˜ ëª©ë¡ê³¼ ì¤‘ë³µ ê°€ëŠ¥
    'ì´ìš©', 'ì°¸ì—¬', # 'ì°¸ì—¬'ëŠ” ì•„ë˜ ëª©ë¡ê³¼ ì¤‘ë³µ

    # --- ì œê°€ ì œì•ˆë“œë¦° í™•ì¥ ëª©ë¡ (ìœ„ ëª©ë¡ê³¼ ì¤‘ë³µë˜ëŠ” ë¶€ë¶„ì€ í†µí•©/ì •ë¦¬) ---
    # 1. ì¼ë°˜ì ì¸ ë¶ˆìš©ì–´ (í™•ì¥)
    'ê³ ', 'í•œ', 'í„°', 'ì´í›„', 'ì´ì „', 'ë‚´', 'ì™¸', 'ì†',
    'ì—´ì‹¬íˆ', # 'ë§¤ìš°', 'ë‹¤ì†Œ', 'íŠ¹íˆ', 'ê°€ì¥', 'ë”ìš±', 'í•­ìƒ', 'ê¾¸ì¤€íˆ'ëŠ” ê¸°ì¡´ ëª©ë¡ì— ìˆìŒ
    'í•˜ë‚˜', 'ë‘˜', 'ì…‹', 'ë„·', 'ë‹¤ì„¯', 'ì—¬ì„¯', 'ì¼ê³±', 'ì—¬ëŸ', 'ì•„í™‰', 'ì—´', # ê¸°ì¡´ ëª©ë¡ì— ìˆ«ì ì—†ìŒ
    'ì²«ì§¸', 'ë‘˜ì§¸', 'ì…‹ì§¸', 'ë‹¤ìŒ', 'ë¨¼ì €', 'ë¹„ë¡¯', 'ë¹„ë¡¯í•œ', 'ë“±ë“±', 'ê¸°íƒ€',

    # 2. ìƒê¸°ë¶€ íŠ¹í™” ë™ì‚¬ì„± ëª…ì‚¬ ë° ì¼ë°˜ ëª…ì‚¬
    'í™œìš©', 'ì‹¤ì‹œ', 'ì§„í–‰', 'ìˆ˜í–‰', 'ì œì‘', 'ê²½í—˜', # 'ì°¸ì—¬', 'ì´ìš©', 'ë…¸ë ¥', 'í•™ìŠµ', 'ì´í•´', 'íƒêµ¬', 'ì—°êµ¬', 'ì¡°ì‚¬', 'ë°œí‘œ', 'í† ë¡ ', 'ë³´ê³ ì„œ', 'ê³¼ì œ', 'ì£¼ì œ', 'ìë£Œ', 'ìˆ˜ì—…', 'ì‹œê°„', 'í”„ë¡œì íŠ¸', 'ì‹¤í—˜' ë“±ì€ ê¸°ì¡´ ëª©ë¡ê³¼ ì¤‘ë³µ ë˜ëŠ” ìœ ì‚¬í•˜ì—¬ í¬í•¨ë¨
    'ê´€ì°°', 'ê¸°ë¡', 'ì •ë¦¬',
    'ì—­ëŸ‰', 'ìì„¸', 'íƒœë„', 'ë°”íƒ•', 'ì—­í• ', 'ê¸°ë°˜', 'í–¥ìƒ', 'ë°œì „', 'ì„±ì¥', # 'ëŠ¥ë ¥'ì€ ê¸°ì¡´ ëª©ë¡ì— ìˆìŒ
    'ìˆ˜ì¤€', 'ê´€ì‹¬', 'í¥ë¯¸', 'í˜¸ê¸°ì‹¬', 'ì§ˆë¬¸', 'ì œì•ˆ', # 'ì ', 'ì¸¡ë©´'ì€ ê¸°ì¡´ ëª©ë¡ì— ìˆìŒ. 'ì œì‹œ'ëŠ” 'ì œì‹œí•¨'ê³¼ ìœ ì‚¬
    'í•´ê²°', 'ë„ì›€', 'í˜‘ë ¥', 'ì†Œí†µ', 'ê´€ê³„', 'ì¤‘ì‹¬', 'ëŒ€ìƒ', 'ë°©ë²•', 'ì›ë¦¬', 'ê°œë…',
    'ì˜ë¯¸', 'ì¤‘ìš”ì„±', 'í•„ìš”ì„±', 'ê°€ì¹˜', 'ë‹¤ì–‘ì„±', 'ì°½ì˜ì„±', 'ì ê·¹ì„±', 'ì„±ì‹¤ì„±', 'ì±…ì„ê°', # 'ì ê·¹ì„±'ì€ 'ì ê·¹ì 'ê³¼ ìœ ì‚¬
    'ìê¸°ì£¼ë„', 'ëª¨ë²”', 'ë¦¬ë”ì‹­', 'íŒ”ë¡œìš°ì‹­', 'ê³µë™ì²´', 'ë°°ë ¤', 'ë‚˜ëˆ”', 'ë´‰ì‚¬',
    'êµê³¼', 'ê³¼ëª©', 'ë‹¨ì›', 'ì˜ì—­', # 'ë¶„ì•¼'ëŠ” ê¸°ì¡´ ëª©ë¡ì— ìˆìŒ
    'í•™ê¸°', 'í•™ë…„', 'í•™êµ', 'êµë‚´', 'êµì™¸',
    'ëŒ€íšŒ', 'í–‰ì‚¬', 'ìº í”„', 'ë™ì•„ë¦¬', 'ë¶€ì„œ', 'ì¡°ì§', 'ë‹¨ì²´', 'ê¸°ê´€', 'ì‹œì„¤',
    'í•™ìƒ', 'êµì‚¬', 'ì¹œêµ¬', 'ìš°ë¦¬', 'ëª¨ë‘ ', 'íŒ€', # 'ìì‹ 'ì€ ê¸°ì¡´ ëª©ë¡ì— ìˆìŒ
    'ì‹œì‘', 'ë§ˆë¬´ë¦¬', 'ì™„ì„±', # 'ì œì¶œ'ì€ ê¸°ì¡´ ëª©ë¡ì— ìˆìŒ
    'í–¥ìƒë¨', 'ë°œì „í•¨', 'ì„±ì¥í•¨', 'ë…¸ë ¥í•¨', 'ì°¸ì—¬í•¨', 'íƒêµ¬í•¨', 'ì—°êµ¬í•¨', 'ë°œí‘œí•¨', # ê¸°ì¡´ ëª©ë¡ê³¼ ì¤‘ë³µë˜ëŠ” ë™ì‚¬ íŒŒìƒ ëª…ì‚¬ ì •ë¦¬
    'ë“œëŸ¬ëƒ„', 'ê°–ì¶¤', 'ì§€ë‹˜', 'ì¸ì •ë¨', 'í™•ì¸ë¨', 'ê´€ì°°ë¨', # 'ë³´ì„'ì€ ê¸°ì¡´ ëª©ë¡ì— ìˆìŒ
    'ìš°ìˆ˜', 'ë›°ì–´ë‚¨', 'íƒì›”', 'ë¯¸í¡', 'ë¶€ì¡±', # 'ìš°ìˆ˜í•¨', 'ë›°ì–´ë‚¨'ì€ ê¸°ì¡´ ëª©ë¡ê³¼ ìœ ì‚¬
    'ê´€ë ¨í•˜ì—¬', 'ëŒ€í•˜ì—¬', 'ë°”íƒ•ìœ¼ë¡œ', 'ì¤‘ì‹¬ìœ¼ë¡œ', 'í†µí•˜ì—¬', 'ë¹„ì¶”ì–´', 'ì•ì„œ',
    'ê¸°ë¡í•¨', 'ê¸°ì¬í•¨', 'ì‘ì„±í•¨',

    # 3. ìƒê¸°ë¶€ ì„œìˆ ì–´ì—ì„œ íŒŒìƒëœ ëª…ì‚¬ (í˜•ì‹ì ì¸ í‘œí˜„)
    'ë¨', 'í•¨', 'ë†’ìŒ', 'ë‚®ìŒ', 'ë§ìŒ', 'ì ìŒ', # 'ìˆìŒ', 'ì—†ìŒ', 'ë³´ì„', 'ì¸ì •ë¨', 'í™•ì¸ë¨', 'ê´€ì°°ë¨' ë“±ì€ ê¸°ì¡´ ë˜ëŠ” ìœ„ ëª©ë¡ê³¼ ì¤‘ë³µ
    'ê¸°ëŒ€ë¨', 'ìš”ë§ë¨'
]
MIN_NOUN_LEN = 2 # ì¶”ì¶œí•  ëª…ì‚¬ì˜ ìµœì†Œ ê¸¸ì´ (í•œ ê¸€ì ëª…ì‚¬ ì œì™¸)
MIN_WORD_COUNT_FOR_W2V = 1 # Word2Vec í•™ìŠµ ì‹œ ë‹¨ì–´ì˜ ìµœì†Œ ë“±ì¥ ë¹ˆë„

# --- ìƒˆë¡œìš´ ì „ì²˜ë¦¬ í•¨ìˆ˜ (KoNLPy Okt ì‚¬ìš©) ---
def extract_meaningful_nouns(text):
    """
    KoNLPy Oktë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì—ì„œ ì˜ë¯¸ ìˆëŠ” ëª…ì‚¬ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    íŠ¹ìˆ˜ë¬¸ì ì œê±°, ë¶ˆìš©ì–´ ì²˜ë¦¬, ì§§ì€ ë‹¨ì–´/ìˆ«ìí˜• ë‹¨ì–´ í•„í„°ë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    # 1. ê¸°ë³¸ì ì¸ íŠ¹ìˆ˜ë¬¸ì ë° ê³µë°± ì •ë¦¬
    text = re.sub(r"[^ê°€-í£ã„±-ã…ã…-ã…£a-zA-Z0-9\s.]+", "", str(text)).strip() # ë§ˆì¹¨í‘œëŠ” ë¬¸ì¥ ë¶„ë¦¬ ìœ„í•´ ìœ ì§€ ì‹œë„
    text = re.sub(r"\s+", " ", text) # ì¤‘ë³µ ê³µë°± ì œê±°

    if not text:
        return []

    # 2. Oktë¥¼ ì‚¬ìš©í•œ ëª…ì‚¬ ì¶”ì¶œ
    nouns = okt.nouns(text)

    # 3. í•„í„°ë§
    meaningful_nouns = []
    for noun in nouns:
        if (
            noun not in STOPWORDS
            and len(noun) >= MIN_NOUN_LEN
            and not noun.isnumeric() # ìˆ«ìë§Œìœ¼ë¡œ ì´ë£¨ì–´ì§„ ë‹¨ì–´ ì œì™¸
        ):
            meaningful_nouns.append(noun)
    return meaningful_nouns

# --- ê¸°ì¡´ í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜ (Counter ì‚¬ìš©) ---
def get_keywords_from_nouns(noun_list):
    """ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸ì—ì„œ ë¹ˆë„ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ í‚¤ì›Œë“œì™€ ë¹ˆë„ìˆ˜ë¥¼ ì •ë ¬í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if not noun_list:
        return [], []
    word_counts = Counter(noun_list)
    # most_common()ì€ (ë‹¨ì–´, ë¹ˆë„ìˆ˜) íŠœí”Œì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜
    sorted_keywords_with_counts = word_counts.most_common()
    
    # í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ì™€ ë¹ˆë„ìˆ˜ ë¦¬ìŠ¤íŠ¸ë¡œ ë¶„ë¦¬
    wordset = [item[0] for item in sorted_keywords_with_counts]
    wordsetcount = [item[1] for item in sorted_keywords_with_counts]
    
    return wordset, wordsetcount


# --- Streamlit UI ---
st.set_page_config(page_title="ìƒê¸°ë¶€ ë¶„ì„ê¸°", layout="wide") # í˜ì´ì§€ ë„“ê²Œ ì‚¬ìš©
st.title("ğŸ“ ìƒê¸°ë¶€ í‚¤ì›Œë“œ ë¶„ì„ ë° ì—°ê´€ ë¬¸ì¥ ì¶”ì²œ")
st.markdown("""
KoNLPy í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª…ì‚¬ ìœ„ì£¼ë¡œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ê³ ,
Word2Vec ëª¨ë¸ì„ í†µí•´ ìœ ì‚¬ ë‹¨ì–´ ë° ê´€ë ¨ ë†’ì€ ë¬¸ì¥ì„ ì°¾ì•„ì¤ë‹ˆë‹¤.
""")

raw_sentence_input = st.text_area("ë¶„ì„í•  ìƒê¸°ë¶€ ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”:", height=250, placeholder="ì—¬ê¸°ì— í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”...")

if st.button("ë¶„ì„ ì‹œì‘ âœ¨"):
    if raw_sentence_input.strip(): # ì…ë ¥ ë‚´ìš©ì´ ì‹¤ì œë¡œ ìˆëŠ”ì§€ í™•ì¸
        with st.spinner('í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš” (KoNLPy ì²« ì‹¤í–‰ ì‹œ ì‹œê°„ì´ ë” ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤) â³'):
            # 1. ì „ì²´ ë¬¸ì„œì—ì„œ ì˜ë¯¸ ìˆëŠ” ëª…ì‚¬ ì¶”ì¶œ (í‚¤ì›Œë“œ ë¶„ì„ìš©)
            all_document_nouns = extract_meaningful_nouns(raw_sentence_input)

            if not all_document_nouns:
                st.error("ë¶„ì„í•  ì˜ë¯¸ ìˆëŠ” ëª…ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤. ì…ë ¥ ë‚´ìš©ì„ í™•ì¸í•˜ê±°ë‚˜ ë¶ˆìš©ì–´ ì„¤ì •ì„ ì ê²€í•´ì£¼ì„¸ìš”.")
            else:
                st.subheader("ğŸ”‘ ì£¼ìš” í‚¤ì›Œë“œ (ëª…ì‚¬, ë¹ˆë„ìˆœ)")
                keywords, keyword_counts = get_keywords_from_nouns(all_document_nouns)
                
                if not keywords:
                    st.warning("í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                else:
                    keyword_df = pd.DataFrame({'í‚¤ì›Œë“œ': keywords, 'ë¹ˆë„ìˆ˜': keyword_counts})
                    st.dataframe(keyword_df.head(15)) # ìƒìœ„ 15ê°œ í‚¤ì›Œë“œ í‘œì‹œ

                    # 2. Word2Vec ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ ë¬¸ì¥ ë‹¨ìœ„ ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸ ì¤€ë¹„
                    # ë§ˆì¹¨í‘œ, ë¬¼ìŒí‘œ, ëŠë‚Œí‘œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì¥ ë¶„ë¦¬ (ë” ì •êµí•œ ë¬¸ì¥ ë¶„ë¦¬ ë¡œì§ì´ í•„ìš”í•  ìˆ˜ ìˆìŒ)
                    raw_sentences = re.split(r'(?<=[.?!])\s+', raw_sentence_input.strip()) # ë¬¸ì¥ êµ¬ë¶„ì ìœ ì§€í•˜ë©° ë¶„ë¦¬
                    
                    sentences_for_w2v = []
                    original_sentences_for_display = [] # ìœ ì‚¬ ë¬¸ì¥ í‘œì‹œ ì‹œ ì›ë³¸ ë¬¸ì¥ ì‚¬ìš© ìœ„í•¨

                    for sentence_text in raw_sentences:
                        sentence_text_cleaned = sentence_text.strip()
                        if sentence_text_cleaned:
                            sentence_nouns = extract_meaningful_nouns(sentence_text_cleaned)
                            if sentence_nouns: # ëª…ì‚¬ê°€ í•˜ë‚˜ë¼ë„ ìˆëŠ” ë¬¸ì¥ë§Œ í•™ìŠµì— ì‚¬ìš©
                                sentences_for_w2v.append(sentence_nouns)
                                original_sentences_for_display.append(sentence_text_cleaned)
                    
                    if not sentences_for_w2v or len(sentences_for_w2v) < 1 : # í•™ìŠµí•  ë¬¸ì¥ì´ ë„ˆë¬´ ì ì€ ê²½ìš°
                        st.error("Word2Vec ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ ë¬¸ì¥(ëª…ì‚¬ ê¸°ë°˜) ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
                    else:
                        try:
                            model = Word2Vec(sentences_for_w2v, vector_size=100, window=5, min_count=MIN_WORD_COUNT_FOR_W2V, workers=4, sg=1)
                            st.success("Word2Vec ëª¨ë¸ í•™ìŠµ ì™„ë£Œ! (ë¬¸ì¥ ë‚´ ëª…ì‚¬ ê¸°ë°˜)")


                            # 3. í‚¤ì›Œë“œì™€ ì—°ê´€ì„± ë†’ì€ ë¬¸ì¥ ì°¾ê¸°
                            st.subheader("ğŸ“œ í‚¤ì›Œë“œì™€ ì—°ê´€ì„± ë†’ì€ ë¬¸ì¥")
                            num_top_sentences = 3 # ê° í‚¤ì›Œë“œë³„ë¡œ ë³´ì—¬ì¤„ ìƒìœ„ ë¬¸ì¥ ìˆ˜
                            displayed_sentence_count = 0

                            for i in range(min(len(keywords), 10)): # ìƒìœ„ 10ê°œ í‚¤ì›Œë“œì— ëŒ€í•´ ì‹œë„
                                if displayed_sentence_count >= 5: # ìµœëŒ€ 5ê°œ í‚¤ì›Œë“œì— ëŒ€í•œ ì—°ê´€ë¬¸ì¥ í‘œì‹œ
                                    break
                                main_keyword = keywords[i]
                                if main_keyword not in model.wv:
                                    # st.write(f"í‚¤ì›Œë“œ '{main_keyword}'ì— ëŒ€í•œ ë²¡í„°ê°€ ì—†ì–´ ë¬¸ì¥ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                                    continue

                                sentence_similarities = []
                                for idx, sentence_nouns in enumerate(sentences_for_w2v): # í•™ìŠµì— ì‚¬ìš©ëœ ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸ ê¸°ì¤€
                                    if not sentence_nouns: # ëª…ì‚¬ê°€ ì—†ëŠ” ë¬¸ì¥ì€ ê±´ë„ˆëœ€
                                        continue

                                    vectors = [model.wv[token] for token in sentence_nouns if token in model.wv]
                                    if not vectors:
                                        # í•´ë‹¹ ë¬¸ì¥ì˜ ëª…ì‚¬ë“¤ì´ ëª¨ë¸ ì–´íœ˜ì— ì—†ëŠ” ê²½ìš°
                                        continue
                                    
                                    sentence_vector = np.mean(vectors, axis=0)
                                    keyword_vector = model.wv[main_keyword]
                                    
                                    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°, 1ì°¨ì› ë°°ì—´ë¡œ ë³€í™˜ í›„ ì ‘ê·¼
                                    similarity_score = cosine_similarity([sentence_vector], [keyword_vector])[0][0]
                                    
                                    # original_sentences_for_display ì—ì„œ ì›ë³¸ ë¬¸ì¥ ê°€ì ¸ì˜¤ê¸°
                                    # sentences_for_w2v ì™€ original_sentences_for_displayëŠ” ê¸¸ì´ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì£¼ì˜.
                                    # ê°€ì¥ ì•ˆì „í•œ ë°©ë²•ì€ sentences_for_w2v ë§Œë“¤ ë•Œ ì›ë³¸ ë¬¸ì¥ë„ ê°™ì´ ì €ì¥í•˜ëŠ” ê²ƒ
                                    # í˜„ì¬ ì½”ë“œëŠ” original_sentences_for_display ì™€ sentences_for_w2v ë¥¼ ë™ê¸°í™” ì‹œí‚´
                                    if idx < len(original_sentences_for_display):
                                      sentence_similarities.append({
                                          'sentence': original_sentences_for_display[idx],
                                          'similarity': similarity_score
                                      })
                                
                                if sentence_similarities:
                                    st.markdown(f"--- \n#### '{main_keyword}' ê´€ë ¨ ë¬¸ì¥:")
                                    sorted_sentences = sorted(sentence_similarities, key=lambda x: x['similarity'], reverse=True)
                                    for item in sorted_sentences[:num_top_sentences]:
                                        st.markdown(f"> {item['sentence']} *(ìœ ì‚¬ë„: {item['similarity']:.3f})*")
                                    displayed_sentence_count +=1
                            if displayed_sentence_count == 0:
                                st.info("ì£¼ìš” í‚¤ì›Œë“œì— ëŒ€í•œ ì—°ê´€ ë¬¸ì¥ì„ ì°¾ì„ ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤.")

                        except Exception as e:
                            st.error(f"Word2Vec ëª¨ë¸ í•™ìŠµ ë˜ëŠ” ìœ ì‚¬ë„ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                            st.error("ì…ë ¥ëœ í…ìŠ¤íŠ¸ì˜ ì–‘ì´ ì¶©ë¶„í•œì§€, ë‹¤ì–‘í•œ ë‹¨ì–´ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
                            st.error("íŠ¹íˆ, Word2Vec ëª¨ë¸ì€ í•™ìŠµ ë°ì´í„°ì˜ ì§ˆê³¼ ì–‘, ê·¸ë¦¬ê³  min_count ì„¤ì •ì— ë¯¼ê°í•©ë‹ˆë‹¤.")
        # ë¶„ì„ ì™„ë£Œ í›„ ìŠ¤í”¼ë„ˆ ìë™ ì¢…ë£Œ
    else:
        st.warning("ë¶„ì„í•  ë‚´ìš©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")

# --- ì‚¬ì´ë“œë°” ---
st.sidebar.header("â„¹ï¸ ì‚¬ìš© ë°©ë²•")
st.sidebar.markdown("""
1.  **ìƒê¸°ë¶€ ë‚´ìš© ì…ë ¥:** ì¤‘ì•™ì˜ í…ìŠ¤íŠ¸ ì…ë ¥ì°½ì— ë¶„ì„í•˜ê³  ì‹¶ì€ ìƒê¸°ë¶€ ë‚´ìš©ì„ ë³µì‚¬í•˜ì—¬ ë¶™ì—¬ë„£ê±°ë‚˜ ì§ì ‘ ì…ë ¥í•©ë‹ˆë‹¤.
2.  **ë¶„ì„ ì‹œì‘:** 'ë¶„ì„ ì‹œì‘ âœ¨' ë²„íŠ¼ì„ í´ë¦­í•©ë‹ˆë‹¤.
3.  **ê²°ê³¼ í™•ì¸:**
    * **ì£¼ìš” í‚¤ì›Œë“œ:** í…ìŠ¤íŠ¸ì—ì„œ ìì£¼ ë“±ì¥í•˜ëŠ” ëª…ì‚¬ë“¤ì´ ë¹ˆë„ìˆœìœ¼ë¡œ í‘œì‹œë©ë‹ˆë‹¤. (ë¶ˆìš©ì–´ ë“± ì œì™¸)
    * **ì—°ê´€ì„± ë†’ì€ ë¬¸ì¥:** ì£¼ìš” í‚¤ì›Œë“œì™€ ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ ë¬¸ì¥ë“¤ì´ ì¶”ì²œë©ë‹ˆë‹¤.

**íŒ:**
* ë” ì •í™•í•œ ë¶„ì„ì„ ìœ„í•´ ì¶©ë¶„í•œ ì–‘ì˜ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
* **ë¶ˆìš©ì–´ ëª©ë¡ (`STOPWORDS`)**ì€ ì•± ì½”ë“œ ë‚´ì—ì„œ ì§ì ‘ ìˆ˜ì •í•˜ì—¬ ë¶„ì„ì˜ ì§ˆì„ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. (í˜„ì¬ëŠ” ê¸°ë³¸ì ì¸ ë‹¨ì–´ë“¤ë§Œ í¬í•¨)
""")
st.sidebar.header("âš™ï¸ ì„¤ì •ê°’ ì •ë³´")
st.sidebar.markdown(f"""
-   ì¶”ì¶œ ëª…ì‚¬ ìµœì†Œ ê¸¸ì´: `{MIN_NOUN_LEN}`
-   Word2Vec ìµœì†Œ ë‹¨ì–´ ë¹ˆë„: `{MIN_WORD_COUNT_FOR_W2V}`
""")
st.sidebar.markdown("---")
st.sidebar.caption("Made with Streamlit & KoNLPy")
