import streamlit as st
import re
import numpy as np
import pandas as pd
import fitz  # PyMuPDF
from gensim.models import Word2Vec
from sklearn.metrics.pairwise import cosine_similarity
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk import pos_tag

# --- NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ (ìµœì´ˆ ì‹¤í–‰ ì‹œ) ---
# Streamlit ì•±ì—ì„œëŠ” ì´ í•¨ìˆ˜ë¥¼ í†µí•´ í•„ìš”í•œ NLTK ë°ì´í„°ë¥¼ ìë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.
@st.cache_resource
def download_nltk_data():
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        nltk.download('punkt')
    try:
        nltk.data.find('corpora/stopwords')
    except LookupError:
        nltk.download('stopwords')
    try:
        nltk.data.find('taggers/averaged_perceptron_tagger')
    except LookupError:
        nltk.download('averaged_perceptron_tagger')
    try:
        nltk.data.find('corpora/wordnet')
    except LookupError:
        nltk.download('wordnet')

download_nltk_data()

# --- ì´ˆê¸° ì„¤ì • ë° ìƒìˆ˜ ì •ì˜ ---

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ì˜ì–´ ì§€ë¬¸ ìƒì„¸ ë¶„ì„ ì—”ì§„", layout="wide")

# ì˜ì–´ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸
STOPWORDS = set(stopwords.words('english'))
# ì¶”ê°€ì ìœ¼ë¡œ ì œì™¸í•˜ê³  ì‹¶ì€ ë‹¨ì–´ê°€ ìˆë‹¤ë©´ ì—¬ê¸°ì— ì¶”ê°€
# STOPWORDS.update(['student', 'school', 'teacher']) 

MIN_WORD_LEN = 2 # ì¶”ì¶œí•  ë‹¨ì–´ì˜ ìµœì†Œ ê¸¸ì´
MIN_WORD_COUNT_FOR_W2V = 1 # Word2Vec í•™ìŠµì„ ìœ„í•œ ë‹¨ì–´ì˜ ìµœì†Œ ë¹ˆë„

# ì˜ì–´ ì—°ê²°ì–´ ë° ì „í™˜ì–´ ì‚¬ì „
CONNECTORS = {
    'Contrast': ['however', 'but', 'in contrast', 'on the other hand', 'conversely', 'nevertheless'],
    'Result': ['therefore', 'as a result', 'consequently', 'thus', 'hence', 'accordingly'],
    'Example': ['for example', 'for instance', 'to illustrate', 'specifically'],
    'Addition': ['and', 'also', 'moreover', 'furthermore', 'in addition', 'besides'],
    'Sequence': ['first', 'second', 'next', 'then', 'finally', 'afterward', 'subsequently']
}

# --- í—¬í¼ í•¨ìˆ˜ ì •ì˜ ---

def extract_text_from_pdf(uploaded_file):
    """PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    text = ""
    try:
        pdf_document = fitz.open(stream=uploaded_file.read(), filetype="pdf")
        for page in pdf_document:
            text += page.get_text()
        pdf_document.close()
    except Exception as e:
        st.error(f"PDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None
    return text

def preprocess_text_english(text):
    """ì…ë ¥ëœ ì˜ì–´ í…ìŠ¤íŠ¸ë¥¼ ì „ì²˜ë¦¬í•˜ì—¬ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ì™€ ê° ë¬¸ì¥ì˜ í•µì‹¬ ë‹¨ì–´(í‘œì œì–´) ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    lemmatizer = WordNetLemmatizer()
    
    # 1. ë¬¸ì¥ ë¶„ë¦¬
    sentences = sent_tokenize(text)
    
    sentence_words_list = []
    for sentence in sentences:
        # ì†Œë¬¸ì ë³€í™˜ ë° íŠ¹ìˆ˜ë¬¸ì ì œê±° (ì•ŒíŒŒë²³, ê³µë°±, ê¸°ë³¸ êµ¬ë‘ì ë§Œ ë‚¨ê¹€)
        cleaned_sentence = re.sub(r"[^a-zA-Z\s]", "", sentence.lower())
        
        # ë‹¨ì–´ í† í°í™”
        words = word_tokenize(cleaned_sentence)
        
        # í’ˆì‚¬ íƒœê¹…
        tagged_words = pos_tag(words)
        
        meaningful_words = []
        for word, tag in tagged_words:
            # í’ˆì‚¬ê°€ ëª…ì‚¬(NN), ë™ì‚¬(VB), í˜•ìš©ì‚¬(JJ)ì¸ ë‹¨ì–´ë§Œ ì„ íƒ
            if tag.startswith('NN') or tag.startswith('VB') or tag.startswith('JJ'):
                # ë¶ˆìš©ì–´ê°€ ì•„ë‹ˆê³ , ê¸¸ì´ê°€ ìµœì†Œ ê¸¸ì´ ì´ìƒì¸ ë‹¨ì–´ë§Œ
                if word not in STOPWORDS and len(word) >= MIN_WORD_LEN:
                    # í‘œì œì–´ ì¶”ì¶œ (ì˜ˆ: running -> run, books -> book)
                    lemmatized_word = lemmatizer.lemmatize(word)
                    meaningful_words.append(lemmatized_word)
        
        sentence_words_list.append(meaningful_words)
        
    return sentences, sentence_words_list

def train_word2vec_model(sentence_words_list):
    """ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ë¡œ Word2Vec ëª¨ë¸ì„ í•™ìŠµì‹œí‚µë‹ˆë‹¤."""
    if not sentence_words_list or len(sentence_words_list) < 1:
        return None
    try:
        model = Word2Vec(sentences=sentence_words_list, vector_size=100, window=5, min_count=MIN_WORD_COUNT_FOR_W2V, workers=4, sg=1)
        return model
    except Exception as e:
        st.error(f"Word2Vec ëª¨ë¸ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

# --- [Part 1 & 2] í•µì‹¬ ë¶„ì„ ì—”ì§„ í•¨ìˆ˜ ---
# (ì´ ë¶€ë¶„ì˜ ë¡œì§ì€ ì´ì „ê³¼ ê±°ì˜ ë™ì¼í•˜ë©°, ì…ë ¥ ë°ì´í„°ë§Œ ì˜ì–´ìš©ìœ¼ë¡œ ë°”ë€œ)
def perform_full_analysis(sentences, sentence_words_list, model):
    """ëª¨ë“  ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    analysis_report = {}
    if not model:
        return {"error": "Word2Vec ëª¨ë¸ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}

    # [Part 1] ì˜ë¯¸ ë²¡í„°í™”
    analysis_report['word_list'] = sorted(list(model.wv.index_to_key))
    analysis_report['word_vector_list'] = [model.wv[word] for word in analysis_report['word_list']]
    analysis_report['sentence_list'] = sentences
    
    sentence_vectors = []
    for words in sentence_words_list:
        vectors = [model.wv[word] for word in words if word in model.wv]
        if vectors:
            sentence_vectors.append(np.mean(vectors, axis=0))
        else:
            sentence_vectors.append(np.zeros(model.vector_size))
    analysis_report['sentence_vector_list'] = sentence_vectors

    all_words = [word for sublist in sentence_words_list for word in sublist]
    valid_vectors = [model.wv[word] for word in all_words if word in model.wv]
    document_vector = np.mean(valid_vectors, axis=0) if valid_vectors else np.zeros(model.vector_size)
    analysis_report['document_vector'] = document_vector

    # [Part 2] ì‹¬ì¸µ ë‚´ìš© ë¶„ì„
    word_sims = cosine_similarity(model.wv.vectors, [document_vector])
    word_sim_pairs = list(zip(model.wv.index_to_key, word_sims.flatten()))
    analysis_report['important_word_list'] = sorted(word_sim_pairs, key=lambda item: item[1], reverse=True)

    sent_sims = cosine_similarity(sentence_vectors, [document_vector])
    sent_sim_pairs = list(zip(sentences, sent_sims.flatten()))
    analysis_report['important_sentence_list'] = sorted(sent_sim_pairs, key=lambda item: item[1], reverse=True)

    vocab_analysis = {}
    top_keywords = [word for word, sim in analysis_report['important_word_list'][:5]]
    for keyword in top_keywords:
        if keyword in model.wv:
            synonyms = model.wv.most_similar(keyword, topn=5)
            vocab_analysis[keyword] = {"Synonyms": synonyms}
    analysis_report['vocabulary_analysis'] = vocab_analysis
    
    if len(sentence_vectors) > 3:
        adj_sent_sims = [cosine_similarity([sentence_vectors[i]], [sentence_vectors[i+1]])[0][0] for i in range(len(sentence_vectors) - 1)]
        split_indices = sorted(range(len(adj_sent_sims)), key=lambda i: adj_sent_sims[i])[:2]
        split_indices.sort()
        
        paragraphs, last_split = [], 0
        for idx in split_indices:
            paragraphs.append(sentences[last_split : idx + 1])
            last_split = idx + 1
        paragraphs.append(sentences[last_split:])
        analysis_report['reconstructed_paragraphs'] = paragraphs
    else:
        analysis_report['reconstructed_paragraphs'] = [sentences]

    found_connectors = []
    for i, sentence in enumerate(sentences):
        for conn_type, conn_list in CONNECTORS.items():
            for conn_word in conn_list:
                # ë‹¨ì–´ ê²½ê³„ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´ ì •ê·œí‘œí˜„ì‹ ì‚¬ìš© (ì˜ˆ: 'and'ê°€ 'sand'ì˜ ì¼ë¶€ë¡œ ì¸ì‹ë˜ëŠ” ê²ƒ ë°©ì§€)
                if re.search(r'\b' + conn_word + r'\b', sentence.lower()):
                    found_connectors.append({"Sentence No.": i + 1, "Sentence": sentence, "Connector": conn_word, "Function": conn_type})
    analysis_report['syntax_analysis'] = {"connectors": found_connectors}

    return analysis_report

# --- [Part 3] ê²°ê³¼ ì¶œë ¥ í•¨ìˆ˜ ---
# (ì¶œë ¥ ë¶€ë¶„ì€ í•œê¸€ë¡œ ìœ ì§€)
def display_report(report):
    """ë¶„ì„ ë¦¬í¬íŠ¸ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°›ì•„ Streamlit UIì— ì²´ê³„ì ìœ¼ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤."""
    st.header("ğŸ“Š ì§€ë¬¸ ìƒì„¸ ë¶„ì„ ê²°ê³¼ ë¦¬í¬íŠ¸")
    
    if "error" in report:
        st.error(report["error"])
        return

    with st.expander("ğŸŒŸ í•µì‹¬ ë‚´ìš© ìš”ì•½", expanded=True):
        col1, col2 = st.columns(2)
        with col1:
            st.subheader("í•µì‹¬ ë‹¨ì–´ (Top 10)")
            df_words = pd.DataFrame(report['important_word_list'][:10], columns=['ë‹¨ì–´ (Word)', 'ë¬¸ì„œ ì „ì²´ì™€ì˜ ê´€ë ¨ë„ (Relevance)'])
            st.dataframe(df_words, use_container_width=True)
        with col2:
            st.subheader("í•µì‹¬ ë¬¸ì¥ (Top 5)")
            for sentence, similarity in report['important_sentence_list'][:5]:
                st.markdown(f"> {sentence} *(ê´€ë ¨ë„: {similarity:.3f})*")

    with st.expander("ğŸ“‘ êµ¬ì¡° ë° êµ¬ë¬¸ ë¶„ì„", expanded=True):
        st.subheader("ë…¼ë¦¬ì  ë¬¸ë‹¨ ì¬êµ¬ì„±")
        st.info("ë¬¸ì¥ ê°„ ì˜ë¯¸ì  ìœ ì‚¬ë„ë¥¼ ë¶„ì„í•˜ì—¬ ë‚´ìš© íë¦„ì´ ë°”ë€ŒëŠ” ì§€ì ì„ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ë‹¨ì„ ì¬êµ¬ì„±í•©ë‹ˆë‹¤.")
        for i, paragraph_sentences in enumerate(report['reconstructed_paragraphs']):
            st.markdown(f"**- ë¬¸ë‹¨ {i+1} -**")
            st.write(" ".join(paragraph_sentences))
            st.markdown("---")
        
        st.subheader("ì—°ê²°ì–´ ë° ì „í™˜ì–´ ì‹ë³„")
        if report['syntax_analysis']['connectors']:
            df_connectors = pd.DataFrame(report['syntax_analysis']['connectors'])
            st.dataframe(df_connectors, use_container_width=True)
        else:
            st.info("ë¶„ì„ ê°€ëŠ¥í•œ ì—°ê²°ì–´ê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    with st.expander("ğŸ” ìƒì„¸ ì–´íœ˜ ë¶„ì„"):
        st.info("ë¬¸ì„œì˜ í•µì‹¬ ë‹¨ì–´ì™€ ì˜ë¯¸ì ìœ¼ë¡œ ê°€ì¥ ìœ ì‚¬í•œ ë‹¨ì–´(ìœ ì˜ì–´)ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.")
        for keyword, analysis in report['vocabulary_analysis'].items():
            st.subheader(f"'{keyword}'ì˜ ë¶„ì„ ê²°ê³¼")
            synonyms_df = pd.DataFrame(analysis['Synonyms'], columns=['ìœ ì‚¬ ë‹¨ì–´ (Similar Word)', 'ìœ ì‚¬ë„ (Similarity)'])
            st.dataframe(synonyms_df)

    with st.expander("ğŸ”¬ Raw ë°ì´í„° ë° ë²¡í„°ê°’ ë³´ê¸°"):
        # ... (ì´ì „ê³¼ ë™ì¼í•œ Raw ë°ì´í„° ì¶œë ¥ ë¡œì§) ...
        pass

# --- ë©”ì¸ UI ë¡œì§ ---
def main():
    st.title("ğŸ“ ì˜ì–´ ì§€ë¬¸ ìƒì„¸ ë¶„ì„ ì—”ì§„")
    st.markdown("ì‚¬ìš©ìê°€ ì…ë ¥í•œ **ì˜ì–´ í…ìŠ¤íŠ¸(ì§€ë¬¸)**ë¥¼ ë‹¤ê°ë„ë¡œ ë¶„ì„í•˜ì—¬ **í•µì‹¬ ë‚´ìš©, êµ¬ì¡°, ì–´íœ˜**ë¥¼ í¬í•¨í•œ ìƒì„¸ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")

    input_method = st.radio("ì…ë ¥ ë°©ì‹ ì„ íƒ", ('í…ìŠ¤íŠ¸ ì§ì ‘ ì…ë ¥', 'PDF íŒŒì¼ ì—…ë¡œë“œ'))
    
    raw_text_input = ""
    if input_method == 'PDF íŒŒì¼ ì—…ë¡œë“œ':
        uploaded_file = st.file_uploader("ë¶„ì„í•  PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.", type="pdf")
        if uploaded_file:
            with st.spinner("PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” ì¤‘..."):
                raw_text_input = extract_text_from_pdf(uploaded_file)
    else:
        raw_text_input = st.text_area("ë¶„ì„í•  ì˜ì–´ ì§€ë¬¸ì„ ì—¬ê¸°ì— ì§ì ‘ ë¶™ì—¬ë„£ìœ¼ì„¸ìš”.", height=250)

    if raw_text_input and raw_text_input.strip():
        if st.button("ë¶„ì„ ì‹œì‘ âœ¨", type="primary"):
            with st.spinner('í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤... (NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œë¡œ ì²« ì‹¤í–‰ ì‹œ ì‹œê°„ì´ ë” ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤) â³'):
                sentences, sentence_words_list = preprocess_text_english(raw_text_input)
                
                if not any(sentence_words_list):
                    st.error("ë¶„ì„í•  ì˜ë¯¸ ìˆëŠ” ë‹¨ì–´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. í…ìŠ¤íŠ¸ ë‚´ìš©ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
                    return

                model = train_word2vec_model(sentence_words_list)

                if model:
                    analysis_report = perform_full_analysis(sentences, sentence_words_list, model)
                    display_report(analysis_report)
                else:
                    st.error("ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ ë¶„ì„ ëª¨ë¸ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.info("ë¶„ì„í•  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ê±°ë‚˜ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")

    st.sidebar.header("â„¹ï¸ í”„ë¡œê·¸ë¨ ì›ë¦¬ (ì˜ì–´)")
    st.sidebar.markdown("""
    1.  **ì…ë ¥ ì „ì²˜ë¦¬**: `NLTK`ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥/ë‹¨ì–´ë¡œ ë‚˜ëˆ„ê³ , ë¶ˆìš©ì–´ë¥¼ ì œê±°í•œ ë’¤ í’ˆì‚¬(ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•µì‹¬ ë‹¨ì–´ë¥¼ ì„ ë³„í•©ë‹ˆë‹¤. ë‹¨ì–´ëŠ” ê¸°ë³¸í˜•(í‘œì œì–´)ìœ¼ë¡œ ë³€í™˜ë©ë‹ˆë‹¤.
    2.  **ì˜ë¯¸ ë²¡í„°í™”**: ì „ì²˜ë¦¬ëœ ë‹¨ì–´ë“¤ì„ `Word2Vec` ëª¨ë¸ë¡œ í•™ìŠµì‹œì¼œ ê° ë‹¨ì–´ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    3.  **ìœ ì‚¬ë„ ë¶„ì„**: ë²¡í„° ê°„ 'ì½”ì‚¬ì¸ ìœ ì‚¬ë„'ë¥¼ ê³„ì‚°í•˜ì—¬ ì˜ë¯¸ì  ê´€ë ¨ì„±ì„ íŒŒì•…í•©ë‹ˆë‹¤.
    4.  **í•µì‹¬ ë‚´ìš©/êµ¬ì¡° ë¶„ì„**: ì´ ìœ ì‚¬ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•µì‹¬ ë‹¨ì–´/ë¬¸ì¥ ì¶”ì¶œ, ë¬¸ë‹¨ ë¶„í•  ë“±ì˜ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """)
    st.sidebar.markdown("---")
    st.sidebar.caption("Made with Streamlit, Gensim & NLTK")

if __name__ == "__main__":
    main()
