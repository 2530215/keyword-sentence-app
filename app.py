import streamlit as st
import re
import numpy as np
import pandas as pd
import fitz  # PyMuPDF
from gensim.models import Word2Vec
from sklearn.metrics.pairwise import cosine_similarity
import nltk # NLTK ë¼ì´ë¸ŒëŸ¬ë¦¬

# --- ================================================================== ---
# ---               ì˜¤ë¥˜ í•´ê²°ì„ ìœ„í•œ í•µì‹¬ ì½”ë“œ (NLTK ì„¤ì •)               ---
# --- =================================----------------================= ---
# Streamlitì˜ ìºì‹œ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ì—¬ ì•± ì„¸ì…˜ë‹¹ ë”± í•œ ë²ˆë§Œ ì‹¤í–‰ë˜ë„ë¡ í•©ë‹ˆë‹¤.
# ì´ë ‡ê²Œ í•˜ë©´ ì•±ì´ ì‹œì‘ë  ë•Œ í•„ìš”í•œ ëª¨ë“  ë°ì´í„°ê°€ ì¤€ë¹„ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ê³ ,
# ì—†ëŠ” ê²½ìš°ì—ë§Œ ë‹¤ìš´ë¡œë“œí•˜ì—¬ LookupErrorë¥¼ ì›ì²œì ìœ¼ë¡œ ë°©ì§€í•©ë‹ˆë‹¤.
@st.cache_resource
def setup_nltk():
    """
    NLTKì˜ í•„ìˆ˜ ë°ì´í„° íŒ¨í‚¤ì§€ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ëŠ” í•¨ìˆ˜.
    ì•± ì‹¤í–‰ ì‹œ ê°€ì¥ ë¨¼ì € í˜¸ì¶œë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
    """
    nltk.download('punkt') # ë¬¸ì¥ í† í°í™”(sent_tokenize)ì— í•„ìš”
    nltk.download('stopwords') # ë¶ˆìš©ì–´(stopwords)ì— í•„ìš”
    nltk.download('averaged_perceptron_tagger') # í’ˆì‚¬ íƒœê¹…(pos_tag)ì— í•„ìš”
    nltk.download('wordnet') # í‘œì œì–´ ì¶”ì¶œ(lemmatize)ì— í•„ìš”

# --- ì•± ì‹¤í–‰ ì‹œ ê°€ì¥ ë¨¼ì € NLTK ì„¤ì •ì„ ìˆ˜í–‰ ---
setup_nltk()
# --- ================================================================== ---


# --- ì´ˆê¸° ì„¤ì • ë° ìƒìˆ˜ ì •ì˜ ---
st.set_page_config(page_title="ì˜ì–´ ì§€ë¬¸ ìƒì„¸ ë¶„ì„ ì—”ì§„", layout="wide")

STOPWORDS = set(stopwords.words('english'))
MIN_WORD_LEN = 2
MIN_WORD_COUNT_FOR_W2V = 1

CONNECTORS = {
    'Contrast': ['however', 'but', 'in contrast', 'on the other hand', 'conversely', 'nevertheless'],
    'Result': ['therefore', 'as a result', 'consequently', 'thus', 'hence', 'accordingly'],
    'Example': ['for example', 'for instance', 'to illustrate', 'specifically'],
    'Addition': ['and', 'also', 'moreover', 'furthermore', 'in addition', 'besides'],
    'Sequence': ['first', 'second', 'next', 'then', 'finally', 'afterward', 'subsequently']
}

# --- í—¬í¼ í•¨ìˆ˜ ì •ì˜ ---

def extract_text_from_pdf(uploaded_file):
    text = ""
    try:
        pdf_document = fitz.open(stream=uploaded_file.read(), filetype="pdf")
        for page in pdf_document:
            text += page.get_text()
        pdf_document.close()
    except Exception as e:
        st.error(f"PDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None
    return text

def preprocess_text_english(text):
    lemmatizer = WordNetLemmatizer()
    sentences = nltk.sent_tokenize(text)
    
    sentence_words_list = []
    for sentence in sentences:
        cleaned_sentence = re.sub(r"[^a-zA-Z\s]", "", sentence.lower())
        words = nltk.word_tokenize(cleaned_sentence)
        tagged_words = nltk.pos_tag(words)
        
        meaningful_words = []
        for word, tag in tagged_words:
            if tag.startswith('NN') or tag.startswith('VB') or tag.startswith('JJ'):
                if word not in STOPWORDS and len(word) >= MIN_WORD_LEN:
                    lemmatized_word = lemmatizer.lemmatize(word)
                    meaningful_words.append(lemmatized_word)
        
        sentence_words_list.append(meaningful_words)
        
    return sentences, sentence_words_list

def train_word2vec_model(sentence_words_list):
    if not sentence_words_list or len(sentence_words_list) < 1:
        return None
    try:
        model = Word2Vec(sentences=sentence_words_list, vector_size=100, window=5, min_count=MIN_WORD_COUNT_FOR_W2V, workers=4, sg=1)
        return model
    except Exception as e:
        st.error(f"Word2Vec ëª¨ë¸ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

def perform_full_analysis(sentences, sentence_words_list, model):
    analysis_report = {}
    if not model:
        return {"error": "Word2Vec ëª¨ë¸ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}

    analysis_report['word_list'] = sorted(list(model.wv.index_to_key))
    analysis_report['word_vector_list'] = [model.wv[word] for word in analysis_report['word_list']]
    analysis_report['sentence_list'] = sentences
    
    sentence_vectors = []
    for words in sentence_words_list:
        vectors = [model.wv[word] for word in words if word in model.wv]
        if vectors:
            sentence_vectors.append(np.mean(vectors, axis=0))
        else:
            sentence_vectors.append(np.zeros(model.vector_size))
    analysis_report['sentence_vector_list'] = sentence_vectors

    all_words = [word for sublist in sentence_words_list for word in sublist]
    valid_vectors = [model.wv[word] for word in all_words if word in model.wv]
    document_vector = np.mean(valid_vectors, axis=0) if valid_vectors else np.zeros(model.vector_size)
    analysis_report['document_vector'] = document_vector

    word_sims = cosine_similarity(model.wv.vectors, [document_vector])
    word_sim_pairs = list(zip(model.wv.index_to_key, word_sims.flatten()))
    analysis_report['important_word_list'] = sorted(word_sim_pairs, key=lambda item: item[1], reverse=True)

    sent_sims = cosine_similarity(sentence_vectors, [document_vector])
    sent_sim_pairs = list(zip(sentences, sent_sims.flatten()))
    analysis_report['important_sentence_list'] = sorted(sent_sim_pairs, key=lambda item: item[1], reverse=True)

    vocab_analysis = {}
    top_keywords = [word for word, sim in analysis_report['important_word_list'][:5]]
    for keyword in top_keywords:
        if keyword in model.wv:
            synonyms = model.wv.most_similar(keyword, topn=5)
            vocab_analysis[keyword] = {"Synonyms": synonyms}
    analysis_report['vocabulary_analysis'] = vocab_analysis
    
    if len(sentence_vectors) > 3:
        adj_sent_sims = [cosine_similarity([sentence_vectors[i]], [sentence_vectors[i+1]])[0][0] for i in range(len(sentence_vectors) - 1)]
        split_indices = sorted(range(len(adj_sent_sims)), key=lambda i: adj_sent_sims[i])[:2]
        split_indices.sort()
        
        paragraphs, last_split = [], 0
        for idx in split_indices:
            paragraphs.append(sentences[last_split : idx + 1])
            last_split = idx + 1
        paragraphs.append(sentences[last_split:])
        analysis_report['reconstructed_paragraphs'] = paragraphs
    else:
        analysis_report['reconstructed_paragraphs'] = [sentences]

    found_connectors = []
    for i, sentence in enumerate(sentences):
        for conn_type, conn_list in CONNECTORS.items():
            for conn_word in conn_list:
                if re.search(r'\b' + conn_word + r'\b', sentence.lower()):
                    found_connectors.append({"Sentence No.": i + 1, "Sentence": sentence, "Connector": conn_word, "Function": conn_type})
    analysis_report['syntax_analysis'] = {"connectors": found_connectors}

    return analysis_report

def display_report(report):
    st.header("ğŸ“Š ì§€ë¬¸ ìƒì„¸ ë¶„ì„ ê²°ê³¼ ë¦¬í¬íŠ¸")
    
    if "error" in report:
        st.error(report["error"])
        return

    with st.expander("ğŸŒŸ í•µì‹¬ ë‚´ìš© ìš”ì•½", expanded=True):
        col1, col2 = st.columns(2)
        with col1:
            st.subheader("í•µì‹¬ ë‹¨ì–´ (Top 10)")
            df_words = pd.DataFrame(report['important_word_list'][:10], columns=['ë‹¨ì–´ (Word)', 'ë¬¸ì„œ ì „ì²´ì™€ì˜ ê´€ë ¨ë„ (Relevance)'])
            st.dataframe(df_words, use_container_width=True)
        with col2:
            st.subheader("í•µì‹¬ ë¬¸ì¥ (Top 5)")
            for sentence, similarity in report['important_sentence_list'][:5]:
                st.markdown(f"> {sentence} *(ê´€ë ¨ë„: {similarity:.3f})*")

    with st.expander("ğŸ“‘ êµ¬ì¡° ë° êµ¬ë¬¸ ë¶„ì„", expanded=True):
        st.subheader("ë…¼ë¦¬ì  ë¬¸ë‹¨ ì¬êµ¬ì„±")
        st.info("ë¬¸ì¥ ê°„ ì˜ë¯¸ì  ìœ ì‚¬ë„ë¥¼ ë¶„ì„í•˜ì—¬ ë‚´ìš© íë¦„ì´ ë°”ë€ŒëŠ” ì§€ì ì„ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ë‹¨ì„ ì¬êµ¬ì„±í•©ë‹ˆë‹¤.")
        for i, paragraph_sentences in enumerate(report['reconstructed_paragraphs']):
            st.markdown(f"**- ë¬¸ë‹¨ {i+1} -**")
            st.write(" ".join(paragraph_sentences))
            st.markdown("---")
        
        st.subheader("ì—°ê²°ì–´ ë° ì „í™˜ì–´ ì‹ë³„")
        if report['syntax_analysis']['connectors']:
            df_connectors = pd.DataFrame(report['syntax_analysis']['connectors'])
            st.dataframe(df_connectors, use_container_width=True)
        else:
            st.info("ë¶„ì„ ê°€ëŠ¥í•œ ì—°ê²°ì–´ê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    with st.expander("ğŸ” ìƒì„¸ ì–´íœ˜ ë¶„ì„"):
        st.info("ë¬¸ì„œì˜ í•µì‹¬ ë‹¨ì–´ì™€ ì˜ë¯¸ì ìœ¼ë¡œ ê°€ì¥ ìœ ì‚¬í•œ ë‹¨ì–´(ìœ ì˜ì–´)ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.")
        for keyword, analysis in report['vocabulary_analysis'].items():
            st.subheader(f"'{keyword}'ì˜ ë¶„ì„ ê²°ê³¼")
            synonyms_df = pd.DataFrame(analysis['Synonyms'], columns=['ìœ ì‚¬ ë‹¨ì–´ (Similar Word)', 'ìœ ì‚¬ë„ (Similarity)'])
            st.dataframe(synonyms_df)

    with st.expander("ğŸ”¬ Raw ë°ì´í„° ë° ë²¡í„°ê°’ ë³´ê¸°"):
        pass

def main():
    st.title("ğŸ“ ì˜ì–´ ì§€ë¬¸ ìƒì„¸ ë¶„ì„ ì—”ì§„")
    st.markdown("ì‚¬ìš©ìê°€ ì…ë ¥í•œ **ì˜ì–´ í…ìŠ¤íŠ¸(ì§€ë¬¸)**ë¥¼ ë‹¤ê°ë„ë¡œ ë¶„ì„í•˜ì—¬ **í•µì‹¬ ë‚´ìš©, êµ¬ì¡°, ì–´íœ˜**ë¥¼ í¬í•¨í•œ ìƒì„¸ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")

    # NLTK ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŒì„ ì‚¬ìš©ìì—ê²Œ ì•Œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (ì„ íƒ ì‚¬í•­)
    st.sidebar.success("ì–¸ì–´ ë¶„ì„ ë¦¬ì†ŒìŠ¤ ì¤€ë¹„ ì™„ë£Œ!")

    input_method = st.radio("ì…ë ¥ ë°©ì‹ ì„ íƒ", ('í…ìŠ¤íŠ¸ ì§ì ‘ ì…ë ¥', 'PDF íŒŒì¼ ì—…ë¡œë“œ'))
    
    raw_text_input = ""
    if input_method == 'PDF íŒŒì¼ ì—…ë¡œë“œ':
        uploaded_file = st.file_uploader("ë¶„ì„í•  PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.", type="pdf")
        if uploaded_file:
            with st.spinner("PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” ì¤‘..."):
                raw_text_input = extract_text_from_pdf(uploaded_file)
    else:
        raw_text_input = st.text_area("ë¶„ì„í•  ì˜ì–´ ì§€ë¬¸ì„ ì—¬ê¸°ì— ì§ì ‘ ë¶™ì—¬ë„£ìœ¼ì„¸ìš”.", height=250)

    if raw_text_input and raw_text_input.strip():
        if st.button("ë¶„ì„ ì‹œì‘ âœ¨", type="primary"):
            with st.spinner('í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤...'):
                # ì´ì œ NLTK í•¨ìˆ˜ë“¤ì€ ì•ˆì „í•˜ê²Œ í˜¸ì¶œë©ë‹ˆë‹¤.
                from nltk.stem import WordNetLemmatizer
                sentences, sentence_words_list = preprocess_text_english(raw_text_input)
                
                if not any(sentence_words_list):
                    st.error("ë¶„ì„í•  ì˜ë¯¸ ìˆëŠ” ë‹¨ì–´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. í…ìŠ¤íŠ¸ ë‚´ìš©ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
                    return

                model = train_word2vec_model(sentence_words_list)

                if model:
                    analysis_report = perform_full_analysis(sentences, sentence_words_list, model)
                    display_report(analysis_report)
                else:
                    st.error("ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ ë¶„ì„ ëª¨ë¸ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.info("ë¶„ì„í•  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ê±°ë‚˜ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")

    st.sidebar.header("â„¹ï¸ í”„ë¡œê·¸ë¨ ì›ë¦¬ (ì˜ì–´)")
    st.sidebar.markdown("""
    1.  **ë¦¬ì†ŒìŠ¤ ì¤€ë¹„**: ì•± ì‹œì‘ ì‹œ `NLTK`ì˜ `punkt`, `stopwords` ë“± í•„ìˆ˜ ë°ì´í„° íŒ¨í‚¤ì§€ë¥¼ ë¨¼ì € ë‹¤ìš´ë¡œë“œí•˜ì—¬ `LookupError`ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.
    2.  **ì…ë ¥ ì „ì²˜ë¦¬**: `NLTK`ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥/ë‹¨ì–´ë¡œ ë‚˜ëˆ„ê³ , ë¶ˆìš©ì–´ë¥¼ ì œê±°í•œ ë’¤ í’ˆì‚¬(ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•µì‹¬ ë‹¨ì–´ë¥¼ ì„ ë³„í•©ë‹ˆë‹¤. ë‹¨ì–´ëŠ” ê¸°ë³¸í˜•(í‘œì œì–´)ìœ¼ë¡œ ë³€í™˜ë©ë‹ˆë‹¤.
    3.  **ì˜ë¯¸ ë²¡í„°í™”**: ì „ì²˜ë¦¬ëœ ë‹¨ì–´ë“¤ì„ `Word2Vec` ëª¨ë¸ë¡œ í•™ìŠµì‹œì¼œ ê° ë‹¨ì–´ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    4.  **ìœ ì‚¬ë„ ë¶„ì„**: ë²¡í„° ê°„ 'ì½”ì‚¬ì¸ ìœ ì‚¬ë„'ë¥¼ ê³„ì‚°í•˜ì—¬ ì˜ë¯¸ì  ê´€ë ¨ì„±ì„ íŒŒì•…í•©ë‹ˆë‹¤.
    5.  **í•µì‹¬ ë‚´ìš©/êµ¬ì¡° ë¶„ì„**: ì´ ìœ ì‚¬ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•µì‹¬ ë‹¨ì–´/ë¬¸ì¥ ì¶”ì¶œ, ë¬¸ë‹¨ ë¶„í•  ë“±ì˜ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """)
    st.sidebar.markdown("---")
    st.sidebar.caption("Made with Streamlit, Gensim & NLTK")

if __name__ == "__main__":
    main()
