from collections import Counter
import streamlit as st
from gensim.models import Word2Vec
import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import re
from konlpy.tag import Okt
import fitz  # PyMuPDF ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸

# --- Okt í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” ---
okt = Okt()

# --- ë¶ˆìš©ì–´ ë° ê¸°ë³¸ ì„¤ì • (ì´ì „ê³¼ ë™ì¼í•˜ê²Œ ìœ ì§€ ë˜ëŠ” ì‚¬ìš©ì ì •ì˜ ëª©ë¡ ì‚¬ìš©) ---
STOPWORDS = [ # ì‚¬ìš©ìì˜ ìµœì‹  ë¶ˆìš©ì–´ ëª©ë¡ìœ¼ë¡œ êµì²´í•´ì£¼ì„¸ìš”
    # ì˜ˆì‹œ: 'ìˆ˜', 'ê²ƒ', 'ë•Œ', 'ë“±', ...
    # (ì´ì „ì— ì œê³µëœ ê¸¸ê³  êµ¬ì²´ì ì¸ ë¶ˆìš©ì–´ ëª©ë¡ì„ ì—¬ê¸°ì— ë„£ì–´ì£¼ì„¸ìš”)
    'ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ë•Œ', 'ë“±', 'ë°', 'ë…„', 'ì›”', 'ì¼', 'ì¢€', 'ì¤‘', 'ìœ„í•´',
    'ê·¸ê²ƒ', 'ì´ê²ƒ', 'ì €ê²ƒ', 'ì—¬ê¸°', 'ì €ê¸°', 'ê±°ê¸°', 'ìì‹ ', 'ìì²´', 'ëŒ€í•œ', 'í†µí•´', 'ê´€ë ¨',
    'ì—¬ëŸ¬', 'ê°€ì§€', 'ë‹¤ë¥¸', 'ë¶€ë¶„', 'ê²½ìš°', 'ì •ë„', 'ì‚¬ì´', 'ë¬¸ì œ', 'ë‚´ìš©', 'ê²°ê³¼', 'ê³¼ì •',
    'ì‚¬ìš©', 'ìƒê°', 'ì§€ê¸ˆ', 'í˜„ì¬', 'ë‹¹ì‹œ', 'ë•Œë¬¸ì—', 'ë©´ì„œ', 'ë™ì•ˆ', 'ìœ„í•œ', 'ë”°ë¼',
    'ëŒ€í•´', 'í†µí•œ', 'ê´€ë ¨ëœ', 'ìˆìŒ', 'ì—†ìŒ', 'ê°™ìŒ', 'ì‚¬í•­', 'í™œë™', 'ëª¨ìŠµ', 'ë¶„ì•¼',
    'ëŠ¥ë ¥', 'ì—­ëŸ‰', 'ìì„¸', 'íƒœë„', 'ë…¸ë ¥', 'ë°”íƒ•', 'ì—­í• ', 'í•™ìŠµ', 'ì´í•´',
    'í•­ìƒ', 'ë§¤ìš°', 'ë‹¤ì†Œ', 'íŠ¹íˆ', 'ê°€ì¥', 'ë”ìš±', 'ì ê·¹ì ', 'êµ¬ì²´ì ', 'ë‹¤ì–‘í•œ', 'ê¾¸ì¤€íˆ',
    'ë›°ì–´ë‚¨', 'ìš°ìˆ˜í•¨', 'ë³´ì„', 'ë°œíœ˜í•¨', 'ì°¸ì—¬í•¨', 'íƒêµ¬í•¨', 'ë°œì „í•¨', 'í–¥ìƒë¨', 'í•¨ì–‘í•¨',
    'ë§Œë“¦', 'ë°œí‘œí•¨', 'ì œì‹œí•¨', 'ì œì¶œí•¨', 'ë°”', 'ì ', 'ì¸¡ë©´', 'ê³¼ì œ', 'ì¡°ì‚¬', 'ì£¼ì œ',
    'ìë£Œ', 'ë°œí‘œ', 'í† ë¡ ', 'ë³´ê³ ì„œ', 'íƒêµ¬', 'ì—°êµ¬', 'í”„ë¡œì íŠ¸', 'ì‹¤í—˜', 'ìˆ˜ì—…', 'ì‹œê°„',
    'ì´ìš©', 'ì°¸ì—¬',
    'ê³ ', 'í•œ', 'í„°', 'ì´í›„', 'ì´ì „', 'ë‚´', 'ì™¸', 'ì†',
    'ì—´ì‹¬íˆ',
    'í•˜ë‚˜', 'ë‘˜', 'ì…‹', 'ë„·', 'ë‹¤ì„¯', 'ì—¬ì„¯', 'ì¼ê³±', 'ì—¬ëŸ', 'ì•„í™‰', 'ì—´',
    'ì²«ì§¸', 'ë‘˜ì§¸', 'ì…‹ì§¸', 'ë‹¤ìŒ', 'ë¨¼ì €', 'ë¹„ë¡¯', 'ë¹„ë¡¯í•œ', 'ë“±ë“±', 'ê¸°íƒ€',
    'í™œìš©', 'ì‹¤ì‹œ', 'ì§„í–‰', 'ìˆ˜í–‰', 'ì œì‘', 'ê²½í—˜',
    'ê´€ì°°', 'ê¸°ë¡', 'ì •ë¦¬',
    'ê¸°ë°˜', 'í–¥ìƒ', 'ë°œì „', 'ì„±ì¥',
    'ìˆ˜ì¤€', 'ê´€ì‹¬', 'í¥ë¯¸', 'í˜¸ê¸°ì‹¬', 'ì§ˆë¬¸', 'ì œì•ˆ',
    'í•´ê²°', 'ë„ì›€', 'í˜‘ë ¥', 'ì†Œí†µ', 'ê´€ê³„', 'ì¤‘ì‹¬', 'ëŒ€ìƒ', 'ë°©ë²•', 'ì›ë¦¬', 'ê°œë…',
    'ì˜ë¯¸', 'ì¤‘ìš”ì„±', 'í•„ìš”ì„±', 'ê°€ì¹˜', 'ë‹¤ì–‘ì„±', 'ì°½ì˜ì„±', 'ì ê·¹ì„±', 'ì„±ì‹¤ì„±', 'ì±…ì„ê°',
    'ìê¸°ì£¼ë„', 'ëª¨ë²”', 'ë¦¬ë”ì‹­', 'íŒ”ë¡œìš°ì‹­', 'ê³µë™ì²´', 'ë°°ë ¤', 'ë‚˜ëˆ”', 'ë´‰ì‚¬',
    'êµê³¼', 'ê³¼ëª©', 'ë‹¨ì›', 'ì˜ì—­',
    'í•™ê¸°', 'í•™ë…„', 'í•™êµ', 'êµë‚´', 'êµì™¸',
    'ëŒ€íšŒ', 'í–‰ì‚¬', 'ìº í”„', 'ë™ì•„ë¦¬', 'ë¶€ì„œ', 'ì¡°ì§', 'ë‹¨ì²´', 'ê¸°ê´€', 'ì‹œì„¤',
    'í•™ìƒ', 'êµì‚¬', 'ì¹œêµ¬', 'ìš°ë¦¬', 'ëª¨ë‘ ', 'íŒ€',
    'ì‹œì‘', 'ë§ˆë¬´ë¦¬', 'ì™„ì„±',
    'ë“œëŸ¬ëƒ„', 'ê°–ì¶¤', 'ì§€ë‹˜', 'ì¸ì •ë¨', 'í™•ì¸ë¨', 'ê´€ì°°ë¨',
    'ìš°ìˆ˜', 'ë›°ì–´ë‚¨', 'íƒì›”', 'ë¯¸í¡', 'ë¶€ì¡±',
    'ê´€ë ¨í•˜ì—¬', 'ëŒ€í•˜ì—¬', 'ë°”íƒ•ìœ¼ë¡œ', 'ì¤‘ì‹¬ìœ¼ë¡œ', 'í†µí•˜ì—¬', 'ë¹„ì¶”ì–´', 'ì•ì„œ',
    'ê¸°ë¡í•¨', 'ê¸°ì¬í•¨', 'ì‘ì„±í•¨',
    'ë¨', 'í•¨', 'ë†’ìŒ', 'ë‚®ìŒ', 'ë§ìŒ', 'ì ìŒ',
    'ê¸°ëŒ€ë¨', 'ìš”ë§ë¨'
]
MIN_NOUN_LEN = 2
MIN_WORD_COUNT_FOR_W2V = 1

# --- PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ í•¨ìˆ˜ ---
def extract_text_from_pdf(uploaded_file):
    """PyMuPDFë¥¼ ì‚¬ìš©í•˜ì—¬ ì—…ë¡œë“œëœ PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    text = ""
    try:
        # ì—…ë¡œë“œëœ íŒŒì¼ì€ BytesIO ê°ì²´ì´ë¯€ë¡œ, streamìœ¼ë¡œ ë°”ë¡œ ì—´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        pdf_document = fitz.open(stream=uploaded_file.read(), filetype="pdf")
        for page_num in range(len(pdf_document)):
            page = pdf_document.load_page(page_num)
            text += page.get_text()
        pdf_document.close()
    except Exception as e:
        st.error(f"PDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        st.error("ì˜¬ë°”ë¥¸ PDF íŒŒì¼ì¸ì§€, ì†ìƒë˜ì§€ ì•Šì•˜ëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”. ì•”í˜¸í™”ëœ PDFëŠ” ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
    return text

# --- ê¸°ì¡´ í•¨ìˆ˜ë“¤ (extract_meaningful_nouns, get_keywords_from_nouns) ì€ ë™ì¼í•˜ê²Œ ì‚¬ìš© ---
def extract_meaningful_nouns(text):
    text = re.sub(r"[^ê°€-í£ã„±-ã…ã…-ã…£a-zA-Z0-9\s.]+", "", str(text)).strip()
    text = re.sub(r"\s+", " ", text)
    if not text: return []
    nouns = okt.nouns(text)
    meaningful_nouns = []
    for noun in nouns:
        if (noun not in STOPWORDS and len(noun) >= MIN_NOUN_LEN and not noun.isnumeric()):
            meaningful_nouns.append(noun)
    return meaningful_nouns

def get_keywords_from_nouns(noun_list):
    if not noun_list: return [], []
    word_counts = Counter(noun_list)
    sorted_keywords_with_counts = word_counts.most_common()
    wordset = [item[0] for item in sorted_keywords_with_counts]
    wordsetcount = [item[1] for item in sorted_keywords_with_counts]
    return wordset, wordsetcount

# --- Streamlit UI ---
st.set_page_config(page_title="ìƒê¸°ë¶€ ë¶„ì„ê¸°", layout="wide")
st.title("ğŸ“ ìƒê¸°ë¶€ í‚¤ì›Œë“œ ë¶„ì„ ë° ì—°ê´€ ë¬¸ì¥ ì¶”ì²œ")
st.markdown("""
KoNLPy í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª…ì‚¬ ìœ„ì£¼ë¡œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ê³ ,
Word2Vec ëª¨ë¸ì„ í†µí•´ ìœ ì‚¬ ë‹¨ì–´ ë° ê´€ë ¨ ë†’ì€ ë¬¸ì¥ì„ ì°¾ì•„ì¤ë‹ˆë‹¤.
**PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê±°ë‚˜ í…ìŠ¤íŠ¸ë¥¼ ì§ì ‘ ì…ë ¥í•˜ì—¬ ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.**
""")

# --- ì…ë ¥ ë°©ì‹ ì„ íƒ ---
# st.subheader("1. ë¶„ì„í•  ìƒê¸°ë¶€ ë°ì´í„° ì…ë ¥") # ì†Œì œëª© ì¶”ê°€
# input_method = st.radio(
#     "ì…ë ¥ ë°©ì‹ì„ ì„ íƒí•˜ì„¸ìš”:",
#     ('í…ìŠ¤íŠ¸ ì§ì ‘ ì…ë ¥', 'PDF íŒŒì¼ ì—…ë¡œë“œ')
# )

raw_sentence_input = None # ì´ˆê¸°í™”
# if input_method == 'í…ìŠ¤íŠ¸ ì§ì ‘ ì…ë ¥':
#     raw_sentence_input_area = st.text_area("ë¶„ì„í•  ìƒê¸°ë¶€ ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”:", height=250, placeholder="ì—¬ê¸°ì— í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”...")
#     if raw_sentence_input_area.strip():
#         raw_sentence_input = raw_sentence_input_area
# else: # PDF íŒŒì¼ ì—…ë¡œë“œ
#     uploaded_pdf_file = st.file_uploader("ìƒê¸°ë¶€ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.", type="pdf")
#     if uploaded_pdf_file is not None:
#         with st.spinner("PDF íŒŒì¼ì„ ì½ê³  ìˆìŠµë‹ˆë‹¤..."):
#             raw_sentence_input = extract_text_from_pdf(uploaded_pdf_file)
#             if raw_sentence_input:
#                 st.success("PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤!")
#                 # ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ì¼ë¶€ë¥¼ ë³´ì—¬ì£¼ì–´ í™•ì¸ (ì„ íƒ ì‚¬í•­)
#                 # st.text_area("ì¶”ì¶œëœ í…ìŠ¤íŠ¸ (ì¼ë¶€):", raw_sentence_input[:1000] + "...", height=100, disabled=True)
#             else:
#                 st.error("PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. íŒŒì¼ ë‚´ìš©ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")

# --- ë” ê°„ë‹¨í•œ ì…ë ¥ ë°©ì‹: íŒŒì¼ ì—…ë¡œë”ì™€ í…ìŠ¤íŠ¸ ì˜ì—­ì„ ëª¨ë‘ í‘œì‹œí•˜ê³ , íŒŒì¼ì´ ìˆìœ¼ë©´ íŒŒì¼ ìš°ì„  ---
st.subheader("1. ë¶„ì„í•  ìƒê¸°ë¶€ ë°ì´í„° ì…ë ¥")
st.markdown("ì´ ë§í¬ëŠ” [ìƒˆ íƒ­ì—ì„œ Naver](https://www.naver.com)ê°€ ì—´ë¦½ë‹ˆë‹¤.", unsafe_allow_html=True) # ì¼ë°˜ ë§ˆí¬ë‹¤ìš´ì€ target ì§€ì› ì•ˆí•¨
uploaded_pdf_file = st.file_uploader("ìƒê¸°ë¶€ PDF íŒŒì¼ ì—…ë¡œë“œ (PDF ì—…ë¡œë“œ ì‹œ ì•„ë˜ í…ìŠ¤íŠ¸ ì…ë ¥ ë‚´ìš©ì€ ë¬´ì‹œë©ë‹ˆë‹¤):", type="pdf")
raw_sentence_input_area = st.text_area("ë˜ëŠ”, ìƒê¸°ë¶€ ë‚´ìš©ì„ ì—¬ê¸°ì— ì§ì ‘ ë¶™ì—¬ë„£ìœ¼ì„¸ìš”:", height=200, placeholder="PDFë¥¼ ì—…ë¡œë“œí•˜ì§€ ì•Šì„ ê²½ìš° ì—¬ê¸°ì— í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”...")

if uploaded_pdf_file is not None:
    with st.spinner("PDF íŒŒì¼ì„ ì½ê³  ë¶„ì„ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤..."):
        extracted_text_from_pdf = extract_text_from_pdf(uploaded_pdf_file)
        if extracted_text_from_pdf:
            raw_sentence_input = extracted_text_from_pdf
            st.success("PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤!")
            # st.info("PDF ë‚´ìš©ìœ¼ë¡œ ë¶„ì„ì„ ì§„í–‰í•©ë‹ˆë‹¤.") # ì‚¬ìš©ìì—ê²Œ ì•Œë¦¼
        else:
            st.error("PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. íŒŒì¼ì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•˜ê±°ë‚˜, ì•„ë˜ í…ìŠ¤íŠ¸ ì˜ì—­ì— ì§ì ‘ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            # PDF ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ í…ìŠ¤íŠ¸ ì˜ì—­ ì…ë ¥ì„ ì‚¬ìš©í•˜ë„ë¡ raw_sentence_inputì„ Noneìœ¼ë¡œ ìœ ì§€
elif raw_sentence_input_area.strip():
    raw_sentence_input = raw_sentence_input_area
    # st.info("ì…ë ¥ëœ í…ìŠ¤íŠ¸ë¡œ ë¶„ì„ì„ ì§„í–‰í•©ë‹ˆë‹¤.")
else:
    # íŒŒì¼ë„ ì—†ê³  í…ìŠ¤íŠ¸ ì…ë ¥ë„ ì—†ëŠ” ê²½ìš°
    pass


# --- ë¶„ì„ ì‹œì‘ ë²„íŠ¼ ë° ë¡œì§ (raw_sentence_inputì´ ì±„ì›Œì¡Œì„ ë•Œë§Œ í™œì„±í™”ë˜ë„ë¡) ---
if raw_sentence_input and raw_sentence_input.strip(): # raw_sentence_inputì´ Noneì´ ì•„ë‹ˆê³ , ê³µë°±ë§Œ ìˆëŠ” ë¬¸ìì—´ì´ ì•„ë‹ ë•Œ
    if st.button("ë¶„ì„ ì‹œì‘ âœ¨"):
        with st.spinner('í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš” (KoNLPy ì²« ì‹¤í–‰ ì‹œ ì‹œê°„ì´ ë” ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤) â³'):
            # (ì´í•˜ ë¶„ì„ ë¡œì§ì€ ì´ì „ê³¼ ê±°ì˜ ë™ì¼. raw_sentence_inputì„ ì‚¬ìš©)
            # 1. ì „ì²´ ë¬¸ì„œì—ì„œ ì˜ë¯¸ ìˆëŠ” ëª…ì‚¬ ì¶”ì¶œ (í‚¤ì›Œë“œ ë¶„ì„ìš©)
            all_document_nouns = extract_meaningful_nouns(raw_sentence_input)

            if not all_document_nouns:
                st.error("ë¶„ì„í•  ì˜ë¯¸ ìˆëŠ” ëª…ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤. ì…ë ¥ ë‚´ìš©ì„ í™•ì¸í•˜ê±°ë‚˜ ë¶ˆìš©ì–´ ì„¤ì •ì„ ì ê²€í•´ì£¼ì„¸ìš”.")
            else:
                st.subheader("ğŸ”‘ ì£¼ìš” í‚¤ì›Œë“œ (ëª…ì‚¬, ë¹ˆë„ìˆœ)")
                keywords, keyword_counts = get_keywords_from_nouns(all_document_nouns)
                
                if not keywords:
                    st.warning("í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                else:
                    keyword_df = pd.DataFrame({'í‚¤ì›Œë“œ': keywords, 'ë¹ˆë„ìˆ˜': keyword_counts})
                    st.dataframe(keyword_df.head(15))

                    # 2. Word2Vec ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ ë¬¸ì¥ ë‹¨ìœ„ ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸ ì¤€ë¹„
                    raw_sentences = re.split(r'(?<=[.?!])\s+', raw_sentence_input.strip())
                    sentences_for_w2v = []
                    original_sentences_for_display = []

                    for sentence_text in raw_sentences:
                        sentence_text_cleaned = sentence_text.strip()
                        if sentence_text_cleaned:
                            sentence_nouns = extract_meaningful_nouns(sentence_text_cleaned)
                            if sentence_nouns:
                                sentences_for_w2v.append(sentence_nouns)
                                original_sentences_for_display.append(sentence_text_cleaned)
                    
                    if not sentences_for_w2v or len(sentences_for_w2v) < 1 :
                        st.error("Word2Vec ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ ë¬¸ì¥(ëª…ì‚¬ ê¸°ë°˜) ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
                    else:
                        try:
                            model = Word2Vec(sentences_for_w2v, vector_size=100, window=5, min_count=MIN_WORD_COUNT_FOR_W2V, workers=4, sg=1)
                            st.success("Word2Vec ëª¨ë¸ í•™ìŠµ ì™„ë£Œ! (ë¬¸ì¥ ë‚´ ëª…ì‚¬ ê¸°ë°˜)")

                            # 3. ì£¼ìš” í‚¤ì›Œë“œì™€ ìœ ì‚¬í•œ ë‹¨ì–´ ì°¾ê¸°
                            st.subheader("ğŸ”— ì£¼ìš” í‚¤ì›Œë“œì™€ ìœ ì‚¬í•œ ë‹¨ì–´ (Word2Vec)")
                            # ... (ì´ì „ ìœ ì‚¬ ë‹¨ì–´ ì°¾ê¸° ë¡œì§ê³¼ ë™ì¼) ...
                            num_similar_words_to_show = 5
                            displayed_similar_count = 0
                            for keyword_to_check in keywords[:10]: 
                                if displayed_similar_count >= 5: 
                                    break
                                if keyword_to_check in model.wv:
                                    similar_words = model.wv.most_similar(keyword_to_check, topn=num_similar_words_to_show)
                                    st.write(f"**'{keyword_to_check}'**ì™€ ìœ ì‚¬í•œ ë‹¨ì–´:")
                                    st.write([f"{word} (ìœ ì‚¬ë„: {similarity:.2f})" for word, similarity in similar_words])
                                    displayed_similar_count +=1
                            if displayed_similar_count == 0:
                                st.info("ì£¼ìš” í‚¤ì›Œë“œì— ëŒ€í•œ ìœ ì‚¬ ë‹¨ì–´ë¥¼ ëª¨ë¸ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤.")

                            # 4. í‚¤ì›Œë“œì™€ ì—°ê´€ì„± ë†’ì€ ë¬¸ì¥ ì°¾ê¸°
                            st.subheader("ğŸ“œ í‚¤ì›Œë“œì™€ ì—°ê´€ì„± ë†’ì€ ë¬¸ì¥")
                            # ... (ì´ì „ ì—°ê´€ ë¬¸ì¥ ì°¾ê¸° ë¡œì§ê³¼ ë™ì¼) ...
                            num_top_sentences = 3 
                            displayed_sentence_count = 0
                            for i in range(min(len(keywords), 10)): 
                                if displayed_sentence_count >= 5: 
                                    break
                                main_keyword = keywords[i]
                                if main_keyword not in model.wv:
                                    continue

                                sentence_similarities = []
                                for idx, sentence_nouns in enumerate(sentences_for_w2v): 
                                    if not sentence_nouns: 
                                        continue
                                    vectors = [model.wv[token] for token in sentence_nouns if token in model.wv]
                                    if not vectors:
                                        continue
                                    sentence_vector = np.mean(vectors, axis=0)
                                    keyword_vector = model.wv[main_keyword]
                                    similarity_score = cosine_similarity([sentence_vector], [keyword_vector])[0][0]
                                    if idx < len(original_sentences_for_display):
                                      sentence_similarities.append({
                                          'sentence': original_sentences_for_display[idx],
                                          'similarity': similarity_score
                                      })
                                if sentence_similarities:
                                    st.markdown(f"--- \n#### '{main_keyword}' ê´€ë ¨ ë¬¸ì¥:")
                                    sorted_sentences = sorted(sentence_similarities, key=lambda x: x['similarity'], reverse=True)
                                    for item in sorted_sentences[:num_top_sentences]:
                                        st.markdown(f"> {item['sentence']} *(ìœ ì‚¬ë„: {item['similarity']:.3f})*")
                                    displayed_sentence_count +=1
                            if displayed_sentence_count == 0:
                                st.info("ì£¼ìš” í‚¤ì›Œë“œì— ëŒ€í•œ ì—°ê´€ ë¬¸ì¥ì„ ì°¾ì„ ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤.")
                        except Exception as e:
                            st.error(f"Word2Vec ëª¨ë¸ í•™ìŠµ ë˜ëŠ” ìœ ì‚¬ë„ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                            # ... (ì˜¤ë¥˜ ë©”ì‹œì§€)
else:
    if not uploaded_pdf_file and not raw_sentence_input_area.strip(): # ì•„ë¬´ê²ƒë„ ì…ë ¥ë˜ì§€ ì•Šì•˜ì„ ë•Œ ì•ˆë‚´
        st.info("ìƒê¸°ë¶€ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê±°ë‚˜, í…ìŠ¤íŠ¸ë¥¼ ì§ì ‘ ì…ë ¥í•´ì£¼ì„¸ìš”.")


# --- ì‚¬ì´ë“œë°” (ì´ì „ê³¼ ë™ì¼) ---
st.sidebar.header("â„¹ï¸ ì‚¬ìš© ë°©ë²•")
st.sidebar.markdown("""
1.  **ìƒê¸°ë¶€ ë°ì´í„° ì…ë ¥:**
    * **PDF íŒŒì¼ ì—…ë¡œë“œ:** 'PDF íŒŒì¼ ì—…ë¡œë“œ' ì„¹ì…˜ì—ì„œ íŒŒì¼ì„ ì„ íƒí•©ë‹ˆë‹¤. (ê¶Œì¥)
    * **í…ìŠ¤íŠ¸ ì§ì ‘ ì…ë ¥:** PDFê°€ ì—†ì„ ê²½ìš°, ì•„ë˜ í…ìŠ¤íŠ¸ ì˜ì—­ì— ë‚´ìš©ì„ ë¶™ì—¬ë„£ìŠµë‹ˆë‹¤.
2.  **ë¶„ì„ ì‹œì‘:** 'ë¶„ì„ ì‹œì‘ âœ¨' ë²„íŠ¼ì„ í´ë¦­í•©ë‹ˆë‹¤. (ë°ì´í„°ê°€ ì…ë ¥ë˜ë©´ ë²„íŠ¼ì´ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.)
3.  **ê²°ê³¼ í™•ì¸:**
    * **ì£¼ìš” í‚¤ì›Œë“œ**, **ìœ ì‚¬ ë‹¨ì–´**, **ì—°ê´€ì„± ë†’ì€ ë¬¸ì¥**ì„ í™•ì¸í•©ë‹ˆë‹¤.

**íŒ:**
* PDF íŒŒì¼ì€ í…ìŠ¤íŠ¸ ê¸°ë°˜ì´ì–´ì•¼ ì •í™•í•œ ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. (ì´ë¯¸ì§€ ìŠ¤ìº” PDFëŠ” ì§€ì› X)
* ë¶ˆìš©ì–´ ëª©ë¡ì€ ì•± ì½”ë“œ ë‚´ì—ì„œ ì§ì ‘ ìˆ˜ì •í•˜ì—¬ ë¶„ì„ì˜ ì§ˆì„ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
""")
# ... (ë‚˜ë¨¸ì§€ ì‚¬ì´ë“œë°” ë‚´ìš©)
st.sidebar.header("âš™ï¸ ì„¤ì •ê°’ ì •ë³´")
st.sidebar.markdown(f"""
-   ì¶”ì¶œ ëª…ì‚¬ ìµœì†Œ ê¸¸ì´: `{MIN_NOUN_LEN}`
-   Word2Vec ìµœì†Œ ë‹¨ì–´ ë¹ˆë„: `{MIN_WORD_COUNT_FOR_W2V}`
""")
st.sidebar.markdown("---")
st.sidebar.caption("Made with Streamlit, KoNLPy & PyMuPDF")
