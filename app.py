from collections import Counter
import streamlit as st
from gensim.models import Word2Vec
import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import re
from konlpy.tag import Okt # KoNLPy Okt ì¶”ê°€

# --- Okt í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” ---
okt = Okt()

# --- ë¶ˆìš©ì–´ ë° ê¸°ë³¸ ì„¤ì • ---
# ë¶ˆìš©ì–´ ëª©ë¡ (ìƒê¸°ë¶€ ë‚´ìš©ì— ë§ì¶° ê³„ì† ì¶”ê°€/ìˆ˜ì •í•˜ì‹œëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤)
STOPWORDS = [
    'ìˆ˜', 'ê²ƒ', 'ë•Œ', 'ë“±', 'ì´', 'ê·¸', 'ì €', 'ë…„', 'ì›”', 'ì¼', 'ì¢€', 'ì¤‘', 'ìœ„í•´', 'ë°',
    'ê·¸ê²ƒ', 'ì´ê²ƒ', 'ì €ê²ƒ', 'ì—¬ê¸°', 'ì €ê¸°', 'ê±°ê¸°', 'ìì‹ ', 'ìì²´', 'ëŒ€í•œ', 'í†µí•´', 'ê´€ë ¨',
    'ì—¬ëŸ¬', 'ê°€ì§€', 'ë‹¤ë¥¸', 'ë¶€ë¶„', 'ê²½ìš°', 'ì •ë„', 'ì‚¬ì´', 'ë¬¸ì œ', 'ë‚´ìš©', 'ê²°ê³¼', 'ê³¼ì •',
    'ì‚¬ìš©', 'ìƒê°', 'ì§€ê¸ˆ', 'í˜„ì¬', 'ë‹¹ì‹œ', 'ë•Œë¬¸ì—', 'ë©´ì„œ', 'ë™ì•ˆ', 'ìœ„í•œ', 'ë”°ë¼',
    'ëŒ€í•´', 'í†µí•œ', 'ê´€ë ¨ëœ', 'ìˆìŒ', 'ì—†ìŒ', 'ê°™ìŒ', 'ì‚¬í•­', 'í™œë™', 'ëª¨ìŠµ', 'ë¶„ì•¼',
    'ëŠ¥ë ¥', 'ëª¨ìŠµ', 'ì—­ëŸ‰', 'ìì„¸', 'íƒœë„', 'ë…¸ë ¥', 'ë°”íƒ•', 'ì—­í• ', 'í•™ìŠµ', 'ì´í•´',
    'í•­ìƒ', 'ë§¤ìš°', 'ë‹¤ì†Œ', 'íŠ¹íˆ', 'ê°€ì¥', 'ë”ìš±', 'ì ê·¹ì ', 'êµ¬ì²´ì ', 'ë‹¤ì–‘í•œ', 'ê¾¸ì¤€íˆ',
    'ë›°ì–´ë‚¨', 'ìš°ìˆ˜í•¨', 'ë³´ì„', 'ë°œíœ˜í•¨', 'ì°¸ì—¬í•¨', 'íƒêµ¬í•¨', 'ë°œì „í•¨', 'í–¥ìƒë¨', 'í•¨ì–‘í•¨',
    'ë§Œë“¦', 'ë°œí‘œí•¨', 'ì œì‹œí•¨', 'ì œì¶œí•¨', 'ë°”', 'ì ', 'ì¸¡ë©´', 'ê³¼ì œ', 'ì¡°ì‚¬', 'ì£¼ì œ',
    'ìë£Œ', 'ë°œí‘œ', 'í† ë¡ ', 'ë³´ê³ ì„œ', 'íƒêµ¬', 'ì—°êµ¬', 'í”„ë¡œì íŠ¸', 'ì‹¤í—˜', 'ìˆ˜ì—…', 'ì‹œê°„'
]
MIN_NOUN_LEN = 2 # ì¶”ì¶œí•  ëª…ì‚¬ì˜ ìµœì†Œ ê¸¸ì´ (í•œ ê¸€ì ëª…ì‚¬ ì œì™¸)
MIN_WORD_COUNT_FOR_W2V = 1 # Word2Vec í•™ìŠµ ì‹œ ë‹¨ì–´ì˜ ìµœì†Œ ë“±ì¥ ë¹ˆë„

# --- ìƒˆë¡œìš´ ì „ì²˜ë¦¬ í•¨ìˆ˜ (KoNLPy Okt ì‚¬ìš©) ---
def extract_meaningful_nouns(text):
    """
    KoNLPy Oktë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì—ì„œ ì˜ë¯¸ ìˆëŠ” ëª…ì‚¬ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    íŠ¹ìˆ˜ë¬¸ì ì œê±°, ë¶ˆìš©ì–´ ì²˜ë¦¬, ì§§ì€ ë‹¨ì–´/ìˆ«ìí˜• ë‹¨ì–´ í•„í„°ë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    # 1. ê¸°ë³¸ì ì¸ íŠ¹ìˆ˜ë¬¸ì ë° ê³µë°± ì •ë¦¬
    text = re.sub(r"[^ê°€-í£ã„±-ã…ã…-ã…£a-zA-Z0-9\s.]+", "", str(text)).strip() # ë§ˆì¹¨í‘œëŠ” ë¬¸ì¥ ë¶„ë¦¬ ìœ„í•´ ìœ ì§€ ì‹œë„
    text = re.sub(r"\s+", " ", text) # ì¤‘ë³µ ê³µë°± ì œê±°

    if not text:
        return []

    # 2. Oktë¥¼ ì‚¬ìš©í•œ ëª…ì‚¬ ì¶”ì¶œ
    nouns = okt.nouns(text)

    # 3. í•„í„°ë§
    meaningful_nouns = []
    for noun in nouns:
        if (
            noun not in STOPWORDS
            and len(noun) >= MIN_NOUN_LEN
            and not noun.isnumeric() # ìˆ«ìë§Œìœ¼ë¡œ ì´ë£¨ì–´ì§„ ë‹¨ì–´ ì œì™¸
        ):
            meaningful_nouns.append(noun)
    return meaningful_nouns

# --- ê¸°ì¡´ í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜ (Counter ì‚¬ìš©) ---
def get_keywords_from_nouns(noun_list):
    """ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸ì—ì„œ ë¹ˆë„ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ í‚¤ì›Œë“œì™€ ë¹ˆë„ìˆ˜ë¥¼ ì •ë ¬í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if not noun_list:
        return [], []
    word_counts = Counter(noun_list)
    # most_common()ì€ (ë‹¨ì–´, ë¹ˆë„ìˆ˜) íŠœí”Œì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜
    sorted_keywords_with_counts = word_counts.most_common()
    
    # í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ì™€ ë¹ˆë„ìˆ˜ ë¦¬ìŠ¤íŠ¸ë¡œ ë¶„ë¦¬
    wordset = [item[0] for item in sorted_keywords_with_counts]
    wordsetcount = [item[1] for item in sorted_keywords_with_counts]
    
    return wordset, wordsetcount


# --- Streamlit UI ---
st.set_page_config(page_title="ìƒê¸°ë¶€ ë¶„ì„ê¸°", layout="wide") # í˜ì´ì§€ ë„“ê²Œ ì‚¬ìš©
st.title("ğŸ“ ìƒê¸°ë¶€ í‚¤ì›Œë“œ ë¶„ì„ ë° ì—°ê´€ ë¬¸ì¥ ì¶”ì²œ")
st.markdown("""
KoNLPy í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª…ì‚¬ ìœ„ì£¼ë¡œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ê³ ,
Word2Vec ëª¨ë¸ì„ í†µí•´ ìœ ì‚¬ ë‹¨ì–´ ë° ê´€ë ¨ ë†’ì€ ë¬¸ì¥ì„ ì°¾ì•„ì¤ë‹ˆë‹¤.
""")

raw_sentence_input = st.text_area("ë¶„ì„í•  ìƒê¸°ë¶€ ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”:", height=250, placeholder="ì—¬ê¸°ì— í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”...")

if st.button("ë¶„ì„ ì‹œì‘ âœ¨"):
    if raw_sentence_input.strip(): # ì…ë ¥ ë‚´ìš©ì´ ì‹¤ì œë¡œ ìˆëŠ”ì§€ í™•ì¸
        with st.spinner('í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš” (KoNLPy ì²« ì‹¤í–‰ ì‹œ ì‹œê°„ì´ ë” ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤) â³'):
            # 1. ì „ì²´ ë¬¸ì„œì—ì„œ ì˜ë¯¸ ìˆëŠ” ëª…ì‚¬ ì¶”ì¶œ (í‚¤ì›Œë“œ ë¶„ì„ìš©)
            all_document_nouns = extract_meaningful_nouns(raw_sentence_input)

            if not all_document_nouns:
                st.error("ë¶„ì„í•  ì˜ë¯¸ ìˆëŠ” ëª…ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤. ì…ë ¥ ë‚´ìš©ì„ í™•ì¸í•˜ê±°ë‚˜ ë¶ˆìš©ì–´ ì„¤ì •ì„ ì ê²€í•´ì£¼ì„¸ìš”.")
            else:
                st.subheader("ğŸ”‘ ì£¼ìš” í‚¤ì›Œë“œ (ëª…ì‚¬, ë¹ˆë„ìˆœ)")
                keywords, keyword_counts = get_keywords_from_nouns(all_document_nouns)
                
                if not keywords:
                    st.warning("í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                else:
                    keyword_df = pd.DataFrame({'í‚¤ì›Œë“œ': keywords, 'ë¹ˆë„ìˆ˜': keyword_counts})
                    st.dataframe(keyword_df.head(15)) # ìƒìœ„ 15ê°œ í‚¤ì›Œë“œ í‘œì‹œ

                    # 2. Word2Vec ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ ë¬¸ì¥ ë‹¨ìœ„ ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸ ì¤€ë¹„
                    # ë§ˆì¹¨í‘œ, ë¬¼ìŒí‘œ, ëŠë‚Œí‘œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì¥ ë¶„ë¦¬ (ë” ì •êµí•œ ë¬¸ì¥ ë¶„ë¦¬ ë¡œì§ì´ í•„ìš”í•  ìˆ˜ ìˆìŒ)
                    raw_sentences = re.split(r'(?<=[.?!])\s+', raw_sentence_input.strip()) # ë¬¸ì¥ êµ¬ë¶„ì ìœ ì§€í•˜ë©° ë¶„ë¦¬
                    
                    sentences_for_w2v = []
                    original_sentences_for_display = [] # ìœ ì‚¬ ë¬¸ì¥ í‘œì‹œ ì‹œ ì›ë³¸ ë¬¸ì¥ ì‚¬ìš© ìœ„í•¨

                    for sentence_text in raw_sentences:
                        sentence_text_cleaned = sentence_text.strip()
                        if sentence_text_cleaned:
                            sentence_nouns = extract_meaningful_nouns(sentence_text_cleaned)
                            if sentence_nouns: # ëª…ì‚¬ê°€ í•˜ë‚˜ë¼ë„ ìˆëŠ” ë¬¸ì¥ë§Œ í•™ìŠµì— ì‚¬ìš©
                                sentences_for_w2v.append(sentence_nouns)
                                original_sentences_for_display.append(sentence_text_cleaned)
                    
                    if not sentences_for_w2v or len(sentences_for_w2v) < 1 : # í•™ìŠµí•  ë¬¸ì¥ì´ ë„ˆë¬´ ì ì€ ê²½ìš°
                        st.error("Word2Vec ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ ë¬¸ì¥(ëª…ì‚¬ ê¸°ë°˜) ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
                    else:
                        try:
                            model = Word2Vec(sentences_for_w2v, vector_size=100, window=5, min_count=MIN_WORD_COUNT_FOR_W2V, workers=4, sg=1)
                            st.success("Word2Vec ëª¨ë¸ í•™ìŠµ ì™„ë£Œ! (ë¬¸ì¥ ë‚´ ëª…ì‚¬ ê¸°ë°˜)")

                            # 3. ì£¼ìš” í‚¤ì›Œë“œì™€ ìœ ì‚¬í•œ ë‹¨ì–´ ì°¾ê¸°
                            st.subheader("ğŸ”— ì£¼ìš” í‚¤ì›Œë“œì™€ ìœ ì‚¬í•œ ë‹¨ì–´ (Word2Vec)")
                            num_similar_words_to_show = 5
                            displayed_similar_count = 0
                            for keyword_to_check in keywords[:10]: # ìƒìœ„ 10ê°œ í‚¤ì›Œë“œì— ëŒ€í•´ ì‹œë„
                                if displayed_similar_count >= 5: # ìµœëŒ€ 5ê°œ í‚¤ì›Œë“œì— ëŒ€í•´ì„œë§Œ ìœ ì‚¬ ë‹¨ì–´ í‘œì‹œ
                                    break
                                if keyword_to_check in model.wv:
                                    similar_words = model.wv.most_similar(keyword_to_check, topn=num_similar_words_to_show)
                                    st.write(f"**'{keyword_to_check}'**ì™€ ìœ ì‚¬í•œ ë‹¨ì–´:")
                                    st.write([f"{word} (ìœ ì‚¬ë„: {similarity:.2f})" for word, similarity in similar_words])
                                    displayed_similar_count +=1
                                # else:
                                #     st.write(f"'{keyword_to_check}'ëŠ”(ì€) í•™ìŠµëœ ëª¨ë¸ì˜ ì–´íœ˜ ì‚¬ì „ì— ì—†ìŠµë‹ˆë‹¤.")
                            if displayed_similar_count == 0:
                                st.info("ì£¼ìš” í‚¤ì›Œë“œì— ëŒ€í•œ ìœ ì‚¬ ë‹¨ì–´ë¥¼ ëª¨ë¸ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤. (ë°ì´í„° ë¶€ì¡± ë˜ëŠ” min_count ì„¤ì • í™•ì¸)")


                            # 4. í‚¤ì›Œë“œì™€ ì—°ê´€ì„± ë†’ì€ ë¬¸ì¥ ì°¾ê¸°
                            st.subheader("ğŸ“œ í‚¤ì›Œë“œì™€ ì—°ê´€ì„± ë†’ì€ ë¬¸ì¥")
                            num_top_sentences = 3 # ê° í‚¤ì›Œë“œë³„ë¡œ ë³´ì—¬ì¤„ ìƒìœ„ ë¬¸ì¥ ìˆ˜
                            displayed_sentence_count = 0

                            for i in range(min(len(keywords), 10)): # ìƒìœ„ 10ê°œ í‚¤ì›Œë“œì— ëŒ€í•´ ì‹œë„
                                if displayed_sentence_count >= 5: # ìµœëŒ€ 5ê°œ í‚¤ì›Œë“œì— ëŒ€í•œ ì—°ê´€ë¬¸ì¥ í‘œì‹œ
                                    break
                                main_keyword = keywords[i]
                                if main_keyword not in model.wv:
                                    # st.write(f"í‚¤ì›Œë“œ '{main_keyword}'ì— ëŒ€í•œ ë²¡í„°ê°€ ì—†ì–´ ë¬¸ì¥ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                                    continue

                                sentence_similarities = []
                                for idx, sentence_nouns in enumerate(sentences_for_w2v): # í•™ìŠµì— ì‚¬ìš©ëœ ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸ ê¸°ì¤€
                                    if not sentence_nouns: # ëª…ì‚¬ê°€ ì—†ëŠ” ë¬¸ì¥ì€ ê±´ë„ˆëœ€
                                        continue

                                    vectors = [model.wv[token] for token in sentence_nouns if token in model.wv]
                                    if not vectors:
                                        # í•´ë‹¹ ë¬¸ì¥ì˜ ëª…ì‚¬ë“¤ì´ ëª¨ë¸ ì–´íœ˜ì— ì—†ëŠ” ê²½ìš°
                                        continue
                                    
                                    sentence_vector = np.mean(vectors, axis=0)
                                    keyword_vector = model.wv[main_keyword]
                                    
                                    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°, 1ì°¨ì› ë°°ì—´ë¡œ ë³€í™˜ í›„ ì ‘ê·¼
                                    similarity_score = cosine_similarity([sentence_vector], [keyword_vector])[0][0]
                                    
                                    # original_sentences_for_display ì—ì„œ ì›ë³¸ ë¬¸ì¥ ê°€ì ¸ì˜¤ê¸°
                                    # sentences_for_w2v ì™€ original_sentences_for_displayëŠ” ê¸¸ì´ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì£¼ì˜.
                                    # ê°€ì¥ ì•ˆì „í•œ ë°©ë²•ì€ sentences_for_w2v ë§Œë“¤ ë•Œ ì›ë³¸ ë¬¸ì¥ë„ ê°™ì´ ì €ì¥í•˜ëŠ” ê²ƒ
                                    # í˜„ì¬ ì½”ë“œëŠ” original_sentences_for_display ì™€ sentences_for_w2v ë¥¼ ë™ê¸°í™” ì‹œí‚´
                                    if idx < len(original_sentences_for_display):
                                      sentence_similarities.append({
                                          'sentence': original_sentences_for_display[idx],
                                          'similarity': similarity_score
                                      })
                                
                                if sentence_similarities:
                                    st.markdown(f"--- \n#### '{main_keyword}' ê´€ë ¨ ë¬¸ì¥:")
                                    sorted_sentences = sorted(sentence_similarities, key=lambda x: x['similarity'], reverse=True)
                                    for item in sorted_sentences[:num_top_sentences]:
                                        st.markdown(f"> {item['sentence']} *(ìœ ì‚¬ë„: {item['similarity']:.3f})*")
                                    displayed_sentence_count +=1
                            if displayed_sentence_count == 0:
                                st.info("ì£¼ìš” í‚¤ì›Œë“œì— ëŒ€í•œ ì—°ê´€ ë¬¸ì¥ì„ ì°¾ì„ ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤.")

                        except Exception as e:
                            st.error(f"Word2Vec ëª¨ë¸ í•™ìŠµ ë˜ëŠ” ìœ ì‚¬ë„ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                            st.error("ì…ë ¥ëœ í…ìŠ¤íŠ¸ì˜ ì–‘ì´ ì¶©ë¶„í•œì§€, ë‹¤ì–‘í•œ ë‹¨ì–´ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
                            st.error("íŠ¹íˆ, Word2Vec ëª¨ë¸ì€ í•™ìŠµ ë°ì´í„°ì˜ ì§ˆê³¼ ì–‘, ê·¸ë¦¬ê³  min_count ì„¤ì •ì— ë¯¼ê°í•©ë‹ˆë‹¤.")
        # ë¶„ì„ ì™„ë£Œ í›„ ìŠ¤í”¼ë„ˆ ìë™ ì¢…ë£Œ
    else:
        st.warning("ë¶„ì„í•  ë‚´ìš©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")

# --- ì‚¬ì´ë“œë°” ---
st.sidebar.header("â„¹ï¸ ì‚¬ìš© ë°©ë²•")
st.sidebar.markdown("""
1.  **ìƒê¸°ë¶€ ë‚´ìš© ì…ë ¥:** ì¤‘ì•™ì˜ í…ìŠ¤íŠ¸ ì…ë ¥ì°½ì— ë¶„ì„í•˜ê³  ì‹¶ì€ ìƒê¸°ë¶€ ë‚´ìš©ì„ ë³µì‚¬í•˜ì—¬ ë¶™ì—¬ë„£ê±°ë‚˜ ì§ì ‘ ì…ë ¥í•©ë‹ˆë‹¤.
2.  **ë¶„ì„ ì‹œì‘:** 'ë¶„ì„ ì‹œì‘ âœ¨' ë²„íŠ¼ì„ í´ë¦­í•©ë‹ˆë‹¤.
3.  **ê²°ê³¼ í™•ì¸:**
    * **ì£¼ìš” í‚¤ì›Œë“œ:** í…ìŠ¤íŠ¸ì—ì„œ ìì£¼ ë“±ì¥í•˜ëŠ” ëª…ì‚¬ë“¤ì´ ë¹ˆë„ìˆœìœ¼ë¡œ í‘œì‹œë©ë‹ˆë‹¤. (ë¶ˆìš©ì–´ ë“± ì œì™¸)
    * **ìœ ì‚¬ ë‹¨ì–´:** ì£¼ìš” í‚¤ì›Œë“œì™€ ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ë‹¨ì–´ë“¤ì´ Word2Vec ëª¨ë¸ì„ í†µí•´ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.
    * **ì—°ê´€ì„± ë†’ì€ ë¬¸ì¥:** ì£¼ìš” í‚¤ì›Œë“œì™€ ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ ë¬¸ì¥ë“¤ì´ ì¶”ì²œë©ë‹ˆë‹¤.

**íŒ:**
* ë” ì •í™•í•œ ë¶„ì„ì„ ìœ„í•´ ì¶©ë¶„í•œ ì–‘ì˜ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
* **ë¶ˆìš©ì–´ ëª©ë¡ (`STOPWORDS`)**ì€ ì•± ì½”ë“œ ë‚´ì—ì„œ ì§ì ‘ ìˆ˜ì •í•˜ì—¬ ë¶„ì„ì˜ ì§ˆì„ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. (í˜„ì¬ëŠ” ê¸°ë³¸ì ì¸ ë‹¨ì–´ë“¤ë§Œ í¬í•¨)
""")
st.sidebar.header("âš™ï¸ ì„¤ì •ê°’ ì •ë³´")
st.sidebar.markdown(f"""
-   ì¶”ì¶œ ëª…ì‚¬ ìµœì†Œ ê¸¸ì´: `{MIN_NOUN_LEN}`
-   Word2Vec ìµœì†Œ ë‹¨ì–´ ë¹ˆë„: `{MIN_WORD_COUNT_FOR_W2V}`
""")
st.sidebar.markdown("---")
st.sidebar.caption("Made with Streamlit & KoNLPy")
'''
from collections import Counter
import streamlit as st
# import urllib.request # ì›¹ì—ì„œ ì§ì ‘ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë¶€ë¶„ì´ ì•„ë‹ˆë¼ë©´ í•„ìš” ì—†ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
from gensim.models import Word2Vec
import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import re # ë¬¸ì¥ ë¶„ë¦¬ ì‹œ ì •ê·œí‘œí˜„ì‹ ì‚¬ìš©

# --- ê¸°ì¡´ ì½”ë“œì˜ í•¨ìˆ˜ë“¤ ---
def processing(input_factor):
    processed_words = []
    for word in input_factor:
        a = word
        alist = list(a)
        if not alist: # ë¹ˆ ë¬¸ìì—´ ì²˜ë¦¬
            continue
        # ì¡°ì‚¬ ë° ì–´ë¯¸ ì œê±° (ë” ì •êµí•œ ì²˜ë¦¬ë¥¼ ìœ„í•´ KoNLPy ê°™ì€ í˜•íƒœì†Œ ë¶„ì„ê¸° ì‚¬ìš©ì„ ê³ ë ¤í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤)
        if (alist[-1]=="ì„")or(alist[-1]=="ë¥¼")or(alist[-1]=="ì€")or(alist[-1]=="ëŠ”")or(alist[-1]=="ì´")or(alist[-1]=="ê°€"):
            processed_words.append("".join(alist[:-1]))
        elif(alist[-1]=="ì™€")or(alist[-1]=="ê³¼"):
            processed_words.append("".join(alist[:-1]))
        elif len(alist) > 1 and ((alist[-2:]==list("ì—ì„œ"))or(alist[-2:]==list("ê»˜ì„œ"))): # ìŠ¬ë¼ì´ì‹± ì˜¤ë¥˜ ë°©ì§€
            processed_words.append("".join(alist[:-2]))
        elif len(alist) > 1 and ((alist[-2:]==list("í•˜ê³ "))or(alist[-2:]==list("í•˜ë‹¤"))or(alist[-2:]==list("í•˜ë©°"))or(alist[-2:]==list("í•˜ëŠ”"))): # ìŠ¬ë¼ì´ì‹± ì˜¤ë¥˜ ë°©ì§€
            processed_words.append("".join(alist[:-2]))
        else:
            processed_words.append(a)
    return [word for word in processed_words if word] # ë¹ˆ ë¬¸ìì—´ ìµœì¢… ì œê±°

def get_keywords(rawword_list):
    wordset = list(set(rawword_list))
    wordsetcount = []
    for element in wordset:
        wordsetcount.append(rawword_list.count(element))

    # ë¹ˆë„ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ë‹¨ì–´ ì •ë ¬ (Counter ê°ì²´ ì‚¬ìš©í•˜ë©´ ë” ê°„ê²°)
    word_counts = Counter(rawword_list)
    sorted_wordset = [word for word, count in word_counts.most_common()]
    sorted_wordsetcount = [count for word, count in word_counts.most_common()]

    return sorted_wordset, sorted_wordsetcount

# --- Streamlit UI ---
st.title("ğŸ“ ìƒê¸°ë¶€ í‚¤ì›Œë“œ ë¶„ì„ ë° ì—°ê´€ ë¬¸ì¥ ì¶”ì²œ")

raw_sentence_input = st.text_area("ë¶„ì„í•  ìƒê¸°ë¶€ ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”:", height=200, placeholder="ì—¬ê¸°ì— í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”...")

if st.button("ë¶„ì„ ì‹œì‘ âœ¨"):
    if raw_sentence_input:
        with st.spinner('í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš” â³'):
            raw_word_list_original = raw_sentence_input.split()
            processed_word_list = processing(list(raw_word_list_original)) # ì›ë³¸ ìœ ì§€ë¥¼ ìœ„í•´ ë³µì‚¬ë³¸ ì‚¬ìš©

            if not processed_word_list:
                st.error("ë¶„ì„í•  ë‹¨ì–´ê°€ ì—†ìŠµë‹ˆë‹¤. í…ìŠ¤íŠ¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
            else:
                st.subheader("ğŸ”‘ ì¶”ì¶œëœ í‚¤ì›Œë“œ (ë¹ˆë„ìˆœ)")
                wordset, wordsetcount = get_keywords(processed_word_list)
                keyword_df = pd.DataFrame({'í‚¤ì›Œë“œ': wordset, 'ë¹ˆë„ìˆ˜': wordsetcount})
                st.dataframe(keyword_df.head(10)) # ìƒìœ„ 10ê°œ í‚¤ì›Œë“œ í‘œì‹œ

                # Word2Vec ëª¨ë¸ í•™ìŠµ
                # Word2Vec í•™ìŠµì„ ìœ„í•´ì„œëŠ” ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤.
                # í˜„ì¬ rawword1ì€ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ë˜ì–´ ìˆëŠ”ë°, gensimì€ ë³´í†µ í† í°í™”ëœ ë¬¸ì¥ë“¤ì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ê¸°ëŒ€í•©ë‹ˆë‹¤.
                # ì˜ˆ: [['ì²«', 'ë¬¸ì¥', 'ë‹¨ì–´ë“¤'], ['ë‘ë²ˆì§¸', 'ë¬¸ì¥', 'ë‹¨ì–´ë“¤']]
                # ì—¬ê¸°ì„œëŠ” ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ë¬¸ì„œë¡œ ë³´ê³ , ê·¸ ì•ˆì˜ ë‹¨ì–´ë“¤ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.
                # ë” ë‚˜ì€ ì„±ëŠ¥ì„ ìœ„í•´ì„œëŠ” ì‹¤ì œ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ í•™ìŠµí•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
                sentences_for_w2v = [processed_word_list] # ë‹¨ì¼ ë¬¸ì„œë¡œ ì·¨ê¸‰
                if not any(sentences_for_w2v): # ëª¨ë“  ë¬¸ì¥ì´ ë¹„ì–´ìˆì§€ ì•Šì€ì§€ í™•ì¸
                    st.error("Word2Vec ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
                else:
                    try:
                        model = Word2Vec(sentences_for_w2v, vector_size=100, window=3, min_count=1, workers=4, sg=1) # sg=1 for skip-gram
                        st.success("Word2Vec ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
                        '''
                        st.subheader("ğŸ”— ì£¼ìš” í‚¤ì›Œë“œì™€ ìœ ì‚¬í•œ ë‹¨ì–´")
                        num_similar_words_to_show = 5
                        # wordsetì—ì„œ ìƒìœ„ Nê°œ í‚¤ì›Œë“œì— ëŒ€í•´ ìœ ì‚¬ ë‹¨ì–´ í‘œì‹œ (ëª¨ë¸ì— ì—†ëŠ” ë‹¨ì–´ ì˜ˆì™¸ ì²˜ë¦¬)
                        for i in range(min(len(wordset), 5)): # ìƒìœ„ 5ê°œ ë˜ëŠ” ê·¸ ì´í•˜
                            keyword_to_check = wordset[i]
                            if keyword_to_check in model.wv:
                                similar_words = model.wv.most_similar(keyword_to_check, topn=num_similar_words_to_show)
                                st.write(f"**'{keyword_to_check}'**ì™€ ìœ ì‚¬í•œ ë‹¨ì–´:")
                                st.write([f"{word} ({similarity:.2f})" for word, similarity in similar_words])
                            else:
                                st.write(f"**'{keyword_to_check}'**ëŠ”(ì€) ëª¨ë¸ì˜ ì–´íœ˜ ì‚¬ì „ì— ì—†ìŠµë‹ˆë‹¤.")
                        '''
                        # ë¬¸ì¥ ìœ ì‚¬ë„ ë¶„ì„
                        st.subheader("ğŸ“œ í‚¤ì›Œë“œì™€ ì—°ê´€ì„± ë†’ì€ ë¬¸ì¥")
                        # ë¬¸ì¥ ë¶„ë¦¬ (ë§ˆì¹¨í‘œ, ë¬¼ìŒí‘œ, ëŠë‚Œí‘œ ê¸°ì¤€)
                        splited_sentences_raw = re.split(r'[.?!]\s*', raw_sentence_input)
                        splited_sentences = [sent.strip() for sent in splited_sentences_raw if sent.strip()]

                        if not splited_sentences:
                            st.warning("ë¶„ì„í•  ë¬¸ì¥ì´ ì—†ìŠµë‹ˆë‹¤.")
                        else:
                            num_top_sentences = 3 # ê° í‚¤ì›Œë“œë³„ë¡œ ë³´ì—¬ì¤„ ìƒìœ„ ë¬¸ì¥ ìˆ˜

                            # ìƒìœ„ í‚¤ì›Œë“œ ëª‡ ê°œì— ëŒ€í•´ì„œë§Œ ë¬¸ì¥ ìœ ì‚¬ë„ ê³„ì‚° (ì˜ˆ: ìƒìœ„ 3ê°œ)
                            for i in range(min(len(wordset), 3)):
                                main_keyword = wordset[i]
                                if main_keyword not in model.wv:
                                    st.write(f"í‚¤ì›Œë“œ '{main_keyword}'ì— ëŒ€í•œ ë²¡í„°ê°€ ì—†ì–´ ë¬¸ì¥ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                                    continue

                                st.markdown(f"--- \n#### '{main_keyword}' ê´€ë ¨ ë¬¸ì¥:")
                                sentence_similarities = []

                                for sentence_text in splited_sentences:
                                    sentence_words_original = sentence_text.split()
                                    processed_sentence_words = processing(list(sentence_words_original))

                                    if not processed_sentence_words:
                                        continue

                                    # ë¬¸ì¥ ë²¡í„° ê³„ì‚° (ëª¨ë¸ì— ìˆëŠ” ë‹¨ì–´ë“¤ë§Œ ì‚¬ìš©)
                                    vectors = [model.wv[token] for token in processed_sentence_words if token in model.wv]
                                    if not vectors:
                                        sentence_similarities.append({'sentence': sentence_text, 'similarity': 0.0})
                                        continue

                                    sentence_vector = np.mean(vectors, axis=0)
                                    keyword_vector = model.wv[main_keyword]
                                    similarity = cosine_similarity([sentence_vector], [keyword_vector])[0][0]
                                    sentence_similarities.append({'sentence': sentence_text, 'similarity': similarity})

                                if sentence_similarities:
                                    # ìœ ì‚¬ë„ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì¥ ì •ë ¬
                                    sorted_sentences = sorted(sentence_similarities, key=lambda x: x['similarity'], reverse=True)
                                    for item in sorted_sentences[:num_top_sentences]:
                                        st.markdown(f"> {item['sentence']} *(ìœ ì‚¬ë„: {item['similarity']:.3f})*")
                                else:
                                    st.write(f"'{main_keyword}'ì™€(ê³¼) ê´€ë ¨ëœ ë¬¸ì¥ì„ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜, ë¬¸ì¥ ë‚´ ë‹¨ì–´ë“¤ì´ ëª¨ë¸ì— ì—†ìŠµë‹ˆë‹¤.")
                    except Exception as e:
                        st.error(f"Word2Vec ëª¨ë¸ í•™ìŠµ ë˜ëŠ” ìœ ì‚¬ë„ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                        st.error("ì…ë ¥ëœ í…ìŠ¤íŠ¸ì˜ ì–‘ì´ ì¶©ë¶„í•œì§€, ë‹¤ì–‘í•œ ë‹¨ì–´ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
                        st.error("íŠ¹íˆ, Word2Vec ëª¨ë¸ì€ í•™ìŠµ ë°ì´í„°ì˜ ì§ˆê³¼ ì–‘ì— ë¯¼ê°í•©ë‹ˆë‹¤.")

    else:
        st.warning("ë¶„ì„í•  ë‚´ìš©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")

st.sidebar.header("â„¹ï¸ ì‚¬ìš© ë°©ë²•")
st.sidebar.markdown("""
1.  **ìƒê¸°ë¶€ ë‚´ìš© ì…ë ¥:** ì¤‘ì•™ì˜ í…ìŠ¤íŠ¸ ì…ë ¥ì°½ì— ë¶„ì„í•˜ê³  ì‹¶ì€ ìƒê¸°ë¶€ ë‚´ìš©ì„ ë³µì‚¬í•˜ì—¬ ë¶™ì—¬ë„£ê±°ë‚˜ ì§ì ‘ ì…ë ¥í•©ë‹ˆë‹¤.
2.  **ë¶„ì„ ì‹œì‘:** 'ë¶„ì„ ì‹œì‘ âœ¨' ë²„íŠ¼ì„ í´ë¦­í•©ë‹ˆë‹¤.
3.  **ê²°ê³¼ í™•ì¸:**
    * **ì¶”ì¶œëœ í‚¤ì›Œë“œ:** í…ìŠ¤íŠ¸ì—ì„œ ìì£¼ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ë“¤ì´ ë¹ˆë„ìˆœìœ¼ë¡œ í‘œì‹œë©ë‹ˆë‹¤.
    * **ì£¼ìš” í‚¤ì›Œë“œì™€ ìœ ì‚¬í•œ ë‹¨ì–´:** ì£¼ìš” í‚¤ì›Œë“œì™€ ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ë‹¨ì–´ë“¤ì´ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.
    * **í‚¤ì›Œë“œì™€ ì—°ê´€ì„± ë†’ì€ ë¬¸ì¥:** ì£¼ìš” í‚¤ì›Œë“œì™€ ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ ë¬¸ì¥ë“¤ì´ ì¶”ì²œë©ë‹ˆë‹¤.

**íŒ:** ë” ì •í™•í•œ ë¶„ì„ì„ ìœ„í•´ ì¶©ë¶„í•œ ì–‘ì˜ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
""")
'''
