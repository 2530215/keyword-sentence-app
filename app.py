from collections import Counter
import streamlit as st
from gensim.models import Word2Vec
import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import re
from konlpy.tag import Okt
import fitz
from github import Github

# --- Okt í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” ---
okt = Okt()

# --- ë¶ˆìš©ì–´ ë° ê¸°ë³¸ ì„¤ì • (ì‚¬ìš©ìë‹˜ì˜ ìµœì‹  ë¶ˆìš©ì–´ ëª©ë¡ ì‚¬ìš©) ---
STOPWORDS = [
    'ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ë•Œ', 'ë“±', 'ë°', 'ë…„', 'ì›”', 'ì¼', 'ì¢€', 'ì¤‘', 'ìœ„í•´',
    'ê·¸ê²ƒ', 'ì´ê²ƒ', 'ì €ê²ƒ', 'ì—¬ê¸°', 'ì €ê¸°', 'ê±°ê¸°', 'ìì‹ ', 'ìì²´', 'ëŒ€í•œ', 'í†µí•´', 'ê´€ë ¨',
    'ì—¬ëŸ¬', 'ê°€ì§€', 'ë‹¤ë¥¸', 'ë¶€ë¶„', 'ê²½ìš°', 'ì •ë„', 'ì‚¬ì´', 'ë¬¸ì œ', 'ë‚´ìš©', 'ê²°ê³¼', 'ê³¼ì •',
    'ì‚¬ìš©', 'ìƒê°', 'ì§€ê¸ˆ', 'í˜„ì¬', 'ë‹¹ì‹œ', 'ë•Œë¬¸ì—', 'ë©´ì„œ', 'ë™ì•ˆ', 'ìœ„í•œ', 'ë”°ë¼',
    'ëŒ€í•´', 'í†µí•œ', 'ê´€ë ¨ëœ', 'ìˆìŒ', 'ì—†ìŒ', 'ê°™ìŒ', 'ì‚¬í•­', 'í™œë™', 'ëª¨ìŠµ', 'ë¶„ì•¼',
    'ëŠ¥ë ¥', 'ì—­ëŸ‰', 'ìì„¸', 'íƒœë„', 'ë…¸ë ¥', 'ë°”íƒ•', 'ì—­í• ', 'í•™ìŠµ', 'ì´í•´',
    'í•­ìƒ', 'ë§¤ìš°', 'ë‹¤ì†Œ', 'íŠ¹íˆ', 'ê°€ì¥', 'ë”ìš±', 'ì ê·¹ì ', 'êµ¬ì²´ì ', 'ë‹¤ì–‘í•œ', 'ê¾¸ì¤€íˆ',
    'ë›°ì–´ë‚¨', 'ìš°ìˆ˜í•¨', 'ë³´ì„', 'ë°œíœ˜í•¨', 'ì°¸ì—¬í•¨', 'íƒêµ¬í•¨', 'ë°œì „í•¨', 'í–¥ìƒë¨', 'í•¨ì–‘í•¨',
    'ë§Œë“¦', 'ë°œí‘œí•¨', 'ì œì‹œí•¨', 'ì œì¶œí•¨', 'ë°”', 'ì ', 'ì¸¡ë©´', 'ê³¼ì œ', 'ì¡°ì‚¬', 'ì£¼ì œ',
    'ìë£Œ', 'ë°œí‘œ', 'í† ë¡ ', 'ë³´ê³ ì„œ', 'íƒêµ¬', 'ì—°êµ¬', 'í”„ë¡œì íŠ¸', 'ì‹¤í—˜', 'ìˆ˜ì—…', 'ì‹œê°„',
    'ì´ìš©', 'ì°¸ì—¬',
    'ê³ ', 'í•œ', 'í„°', 'ì´í›„', 'ì´ì „', 'ë‚´', 'ì™¸', 'ì†',
    'ì—´ì‹¬íˆ',
    'í•˜ë‚˜', 'ë‘˜', 'ì…‹', 'ë„·', 'ë‹¤ì„¯', 'ì—¬ì„¯', 'ì¼ê³±', 'ì—¬ëŸ', 'ì•„í™‰', 'ì—´',
    'ì²«ì§¸', 'ë‘˜ì§¸', 'ì…‹ì§¸', 'ë‹¤ìŒ', 'ë¨¼ì €', 'ë¹„ë¡¯', 'ë¹„ë¡¯í•œ', 'ë“±ë“±', 'ê¸°íƒ€',
    'í™œìš©', 'ì‹¤ì‹œ', 'ì§„í–‰', 'ìˆ˜í–‰', 'ì œì‘', 'ê²½í—˜',
    'ê´€ì°°', 'ê¸°ë¡', 'ì •ë¦¬',
    'ê¸°ë°˜', 'í–¥ìƒ', 'ë°œì „', 'ì„±ì¥',
    'ìˆ˜ì¤€', 'ê´€ì‹¬', 'í¥ë¯¸', 'í˜¸ê¸°ì‹¬', 'ì§ˆë¬¸', 'ì œì•ˆ',
    'í•´ê²°', 'ë„ì›€', 'í˜‘ë ¥', 'ì†Œí†µ', 'ê´€ê³„', 'ì¤‘ì‹¬', 'ëŒ€ìƒ', 'ë°©ë²•', 'ì›ë¦¬', 'ê°œë…',
    'ì˜ë¯¸', 'ì¤‘ìš”ì„±', 'í•„ìš”ì„±', 'ê°€ì¹˜', 'ë‹¤ì–‘ì„±', 'ì°½ì˜ì„±', 'ì ê·¹ì„±', 'ì„±ì‹¤ì„±', 'ì±…ì„ê°',
    'ìê¸°ì£¼ë„', 'ëª¨ë²”', 'ë¦¬ë”ì‹­', 'íŒ”ë¡œìš°ì‹­', 'ê³µë™ì²´', 'ë°°ë ¤', 'ë‚˜ëˆ”', 'ë´‰ì‚¬',
    'êµê³¼', 'ê³¼ëª©', 'ë‹¨ì›', 'ì˜ì—­',
    'í•™ê¸°', 'í•™ë…„', 'í•™êµ', 'êµë‚´', 'êµì™¸',
    'ëŒ€íšŒ', 'í–‰ì‚¬', 'ìº í”„', 'ë™ì•„ë¦¬', 'ë¶€ì„œ', 'ì¡°ì§', 'ë‹¨ì²´', 'ê¸°ê´€', 'ì‹œì„¤',
    'í•™ìƒ', 'êµì‚¬', 'ì¹œêµ¬', 'ìš°ë¦¬', 'ëª¨ë‘ ', 'íŒ€',
    'ì‹œì‘', 'ë§ˆë¬´ë¦¬', 'ì™„ì„±',
    'ë“œëŸ¬ëƒ„', 'ê°–ì¶¤', 'ì§€ë‹˜', 'ì¸ì •ë¨', 'í™•ì¸ë¨', 'ê´€ì°°ë¨',
    'ìš°ìˆ˜', 'ë›°ì–´ë‚¨', 'íƒì›”', 'ë¯¸í¡', 'ë¶€ì¡±',
    'ê´€ë ¨í•˜ì—¬', 'ëŒ€í•˜ì—¬', 'ë°”íƒ•ìœ¼ë¡œ', 'ì¤‘ì‹¬ìœ¼ë¡œ', 'í†µí•˜ì—¬', 'ë¹„ì¶”ì–´', 'ì•ì„œ',
    'ê¸°ë¡í•¨', 'ê¸°ì¬í•¨', 'ì‘ì„±í•¨',
    'ë¨', 'í•¨', 'ë†’ìŒ', 'ë‚®ìŒ', 'ë§ìŒ', 'ì ìŒ',
    'ê¸°ëŒ€ë¨', 'ìš”ë§ë¨'
    'ìƒì›','ê³ ë“±í•™êµ','ìƒì›ê³ ë“±í•™êµ','ë²ˆí˜¸','í‘œí˜„','ì„¤ëª…','í‘œí˜„'
]
MIN_NOUN_LEN = 2
MIN_WORD_COUNT_FOR_W2V = 1

# --- PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ í•¨ìˆ˜ (ì´ì „ê³¼ ë™ì¼) ---
def extract_text_from_pdf(uploaded_file):
    text = ""
    try:
        pdf_document = fitz.open(stream=uploaded_file.read(), filetype="pdf")
        for page_num in range(len(pdf_document)):
            page = pdf_document.load_page(page_num)
            text += page.get_text()
        pdf_document.close()
    except Exception as e:
        st.error(f"PDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None
    return text

# --- ëª…ì‚¬ ì¶”ì¶œ í•¨ìˆ˜ (ì´ì „ê³¼ ë™ì¼) ---
def extract_meaningful_nouns(text):
    text = re.sub(r"[^ê°€-í£ã„±-ã…ã…-ã…£a-zA-Z0-9\s.]+", "", str(text)).strip()
    text = re.sub(r"\s+", " ", text)
    if not text: return []
    nouns = okt.nouns(text)
    meaningful_nouns = []
    for noun in nouns:
        if (noun not in STOPWORDS and len(noun) >= MIN_NOUN_LEN and not noun.isnumeric()):
            meaningful_nouns.append(noun)
    return meaningful_nouns

# --- ë¹ˆë„ìˆ˜ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜ (ì´ì „ê³¼ ë™ì¼) ---
def get_keywords_from_nouns_by_freq(noun_list): # í•¨ìˆ˜ ì´ë¦„ ë³€ê²½
    if not noun_list: return [], []
    word_counts = Counter(noun_list)
    sorted_keywords_with_counts = word_counts.most_common()
    wordset = [item[0] for item in sorted_keywords_with_counts]
    wordsetcount = [item[1] for item in sorted_keywords_with_counts]
    return wordset, wordsetcount

# --- Streamlit UI (ì¼ë¶€ë§Œ í‘œì‹œ, í•µì‹¬ ë¡œì§ ìœ„ì£¼) ---
st.set_page_config(page_title="ìƒê¸°ë¶€ ë¶„ì„ê¸°", layout="wide")
st.title("ğŸ“ ìƒê¸°ë¶€ í‚¤ì›Œë“œ ë¶„ì„ ë° ì—°ê´€ ë¬¸ì¥ ì¶”ì²œ")
# ... (UI ìƒë‹¨ ë§ˆí¬ë‹¤ìš´, íŒŒì¼ ì—…ë¡œë“œ, í…ìŠ¤íŠ¸ ì…ë ¥ ë¶€ë¶„ì€ ì´ì „ê³¼ ê±°ì˜ ë™ì¼í•˜ê²Œ ìœ ì§€) ...
st.markdown("""
KoNLPy í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª…ì‚¬ ìœ„ì£¼ë¡œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ê³ ,
Word2Vec ëª¨ë¸ì„ í†µí•´ ìœ ì‚¬ ë‹¨ì–´ ë° ê´€ë ¨ ë†’ì€ ë¬¸ì¥ì„ ì°¾ì•„ì¤ë‹ˆë‹¤.
**PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê±°ë‚˜ í…ìŠ¤íŠ¸ë¥¼ ì§ì ‘ ì…ë ¥í•˜ì—¬ ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.**
""")
st.subheader("1. ë¶„ì„í•  ìƒê¸°ë¶€ ë°ì´í„° ì…ë ¥")
st.markdown("[ì •ë¶€24ì—ì„œ ìƒê¸°ë¶€ pdf ë‹¤ìš´ë°›ëŠ”ë²•](https://blog.naver.com/leeyju4/223208661500)", unsafe_allow_html=True)
st.markdown("[ì¹´ì¹´ì˜¤í†¡ì—ì„œ ìƒê¸°ë¶€ pdf ë‹¤ìš´ë°›ëŠ”ë²•](https://blog.naver.com/needtime0514/223256443411)", unsafe_allow_html=True)

uploaded_pdf_file = st.file_uploader("ìƒê¸°ë¶€ PDF íŒŒì¼ ì—…ë¡œë“œ (PDF ì—…ë¡œë“œ ì‹œ ì•„ë˜ í…ìŠ¤íŠ¸ ì…ë ¥ ë‚´ìš©ì€ ë¬´ì‹œë©ë‹ˆë‹¤):", type="pdf")
raw_sentence_input_area = st.text_area("ë˜ëŠ”, ìƒê¸°ë¶€ ë‚´ìš©ì„ ì—¬ê¸°ì— ì§ì ‘ ë¶™ì—¬ë„£ìœ¼ì„¸ìš”:", height=200, placeholder="PDFë¥¼ ì—…ë¡œë“œí•˜ì§€ ì•Šì„ ê²½ìš° ì—¬ê¸°ì— í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”...")

raw_sentence_input = None
if uploaded_pdf_file is not None:
    with st.spinner("PDF íŒŒì¼ì„ ì½ê³  ë¶„ì„ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤..."):
        extracted_text_from_pdf = extract_text_from_pdf(uploaded_pdf_file)
        if extracted_text_from_pdf:
            raw_sentence_input = extracted_text_from_pdf
            st.success("PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤!")
        else:
            st.error("PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. íŒŒì¼ì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•˜ê±°ë‚˜, ì•„ë˜ í…ìŠ¤íŠ¸ ì˜ì—­ì— ì§ì ‘ ì…ë ¥í•´ì£¼ì„¸ìš”.")
elif raw_sentence_input_area.strip():
    raw_sentence_input = raw_sentence_input_area
else:
    pass

if raw_sentence_input and raw_sentence_input.strip():
    if st.button("ë¶„ì„ ì‹œì‘ âœ¨"):
        with st.spinner('í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤... (KoNLPy/Word2Vec ì²« ì‹¤í–‰ ì‹œ ì‹œê°„ì´ ë” ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤) â³'):
            all_document_nouns = extract_meaningful_nouns(raw_sentence_input)

            if not all_document_nouns:
                st.error("ë¶„ì„í•  ì˜ë¯¸ ìˆëŠ” ëª…ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤. ì…ë ¥ ë‚´ìš©ì„ í™•ì¸í•˜ê±°ë‚˜ ë¶ˆìš©ì–´ ì„¤ì •ì„ ì ê²€í•´ì£¼ì„¸ìš”.")
            else:
                # --- 1. ë¹ˆë„ìˆ˜ ê¸°ë°˜ í‚¤ì›Œë“œ í‘œì‹œ (ê¸°ì¡´ ë°©ì‹) ---
                st.subheader("ğŸ”‘ ì£¼ìš” í‚¤ì›Œë“œ (ë‹¨ìˆœ ë¹ˆë„ìˆ˜ ê¸°ë°˜)")
                keywords_freq, keyword_counts_freq = get_keywords_from_nouns_by_freq(all_document_nouns)
                if not keywords_freq:
                    st.warning("ë¹ˆë„ìˆ˜ ê¸°ë°˜ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                else:
                    keyword_df_freq = pd.DataFrame({'í‚¤ì›Œë“œ': keywords_freq, 'ë¹ˆë„ìˆ˜': keyword_counts_freq})
                    st.dataframe(keyword_df_freq.head(10)) # ìƒìœ„ 10ê°œ í‘œì‹œ

                # --- Word2Vec ëª¨ë¸ í•™ìŠµ (ì´ì „ê³¼ ë™ì¼) ---
                raw_sentences = re.split(r'(?<=[.?!])\s+', raw_sentence_input.strip())
                sentences_for_w2v = []
                original_sentences_for_display = []
                for sentence_text in raw_sentences:
                    sentence_text_cleaned = sentence_text.strip()
                    if sentence_text_cleaned:
                        sentence_nouns = extract_meaningful_nouns(sentence_text_cleaned)
                        if sentence_nouns:
                            sentences_for_w2v.append(sentence_nouns)
                            original_sentences_for_display.append(sentence_text_cleaned)
                
                if not sentences_for_w2v or len(sentences_for_w2v) < 1:
                    st.error("Word2Vec ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ ë¬¸ì¥(ëª…ì‚¬ ê¸°ë°˜) ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
                    # ì˜ë¯¸ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œì€ Word2Vec ëª¨ë¸ì´ í•„ìš”í•˜ë¯€ë¡œ ì—¬ê¸°ì„œ ì¤‘ë‹¨ë  ìˆ˜ ìˆìŒ
                    model = None # ëª¨ë¸ì´ ì—†ìŒì„ ëª…ì‹œ
                else:
                    try:
                        model = Word2Vec(sentences_for_w2v, vector_size=100, window=5, min_count=MIN_WORD_COUNT_FOR_W2V, workers=4, sg=1)
                        st.success("Word2Vec ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
                    except Exception as e:
                        st.error(f"Word2Vec ëª¨ë¸ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                        model = None # ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨

                # --- 2. ì˜ë¯¸ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ (ë¬¸ì„œ ë²¡í„°ì™€ ìœ ì‚¬ë„) ---
                if model: # Word2Vec ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ í•™ìŠµëœ ê²½ìš°ì—ë§Œ ì‹¤í–‰
                    st.subheader("ğŸŒŸ ì£¼ìš” í‚¤ì›Œë“œ (ë¬¸ì„œ ì „ì²´ ì˜ë¯¸ ê¸°ë°˜)")
                    
                    # 2a. ë¬¸ì„œ ëŒ€í‘œ ë²¡í„° ê³„ì‚°
                    doc_vector_sum = np.zeros(model.vector_size)
                    word_count_for_doc_vector = 0
                    # all_document_nouns ì¤‘ì—ì„œ ëª¨ë¸ ì–´íœ˜ì— ìˆëŠ” ë‹¨ì–´ë“¤ë§Œ ì‚¬ìš©
                    valid_nouns_for_doc_vector = [noun for noun in all_document_nouns if noun in model.wv]
                    
                    if not valid_nouns_for_doc_vector:
                        st.warning("ë¬¸ì„œ ëŒ€í‘œ ë²¡í„°ë¥¼ ê³„ì‚°í•  ë‹¨ì–´ê°€ ëª¨ë¸ì— ì—†ìŠµë‹ˆë‹¤.")
                    else:
                        for word in valid_nouns_for_doc_vector:
                            doc_vector_sum += model.wv[word]
                            word_count_for_doc_vector += 1
                        
                        if word_count_for_doc_vector > 0:
                            document_vector = doc_vector_sum / word_count_for_doc_vector
                            
                            # 2b. ê° ê³ ìœ  ëª…ì‚¬ì™€ ë¬¸ì„œ ëŒ€í‘œ ë²¡í„° ê°„ ìœ ì‚¬ë„ ê³„ì‚°
                            # í‚¤ì›Œë“œ í›„ë³´ëŠ” all_document_nounsì˜ ê³ ìœ í•œ ëª…ì‚¬ë“¤ ì¤‘ ëª¨ë¸ì— ìˆëŠ” ê²ƒë“¤
                            candidate_keywords = sorted(list(set(valid_nouns_for_doc_vector))) # ê³ ìœ  ëª…ì‚¬ ì •ë ¬
                            
                            keyword_similarities_to_doc = []
                            for keyword_candidate in candidate_keywords:
                                try:
                                    similarity = cosine_similarity([model.wv[keyword_candidate]], [document_vector])[0][0]
                                    keyword_similarities_to_doc.append((keyword_candidate, similarity))
                                except KeyError:
                                    # ì´ë¡ ìƒ valid_nouns_for_doc_vectorì— ìˆìœ¼ë¯€ë¡œ ì´ ì—ëŸ¬ëŠ” ì•ˆë‚˜ì•¼ í•¨
                                    continue 
                            
                            # 2c. ìœ ì‚¬ë„ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬ ë° í‘œì‹œ
                            if keyword_similarities_to_doc:
                                sorted_keywords_by_meaning = sorted(keyword_similarities_to_doc, key=lambda item: item[1], reverse=True)
                                
                                keywords_meaning = [item[0] for item in sorted_keywords_by_meaning]
                                keyword_scores_meaning = [item[1] for item in sorted_keywords_by_meaning]
                                
                                keyword_df_meaning = pd.DataFrame({'í‚¤ì›Œë“œ': keywords_meaning, 'ë¬¸ì„œ ëŒ€í‘œ ë²¡í„°ì™€ì˜ ìœ ì‚¬ë„': keyword_scores_meaning})
                                st.dataframe(keyword_df_meaning.head(15)) # ìƒìœ„ 15ê°œ í‘œì‹œ
                            else:
                                st.warning("ì˜ë¯¸ ê¸°ë°˜ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                        else:
                             st.warning("ë¬¸ì„œ ëŒ€í‘œ ë²¡í„° ê³„ì‚°ì— ì‚¬ìš©ë  ìœ íš¨í•œ ë‹¨ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    st.warning("Word2Vec ëª¨ë¸ì´ ì—†ì–´ ì˜ë¯¸ ê¸°ë°˜ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

                # --- 3. ì£¼ìš” í‚¤ì›Œë“œì™€ ìœ ì‚¬í•œ ë‹¨ì–´ ì°¾ê¸° (Word2Vec) ---
                if model:
                    st.subheader("ğŸ”— ìœ ì‚¬ ë‹¨ì–´ (Word2Vec)")
                    # ìœ ì‚¬ ë‹¨ì–´ ì°¾ê¸°ì˜ ëŒ€ìƒ í‚¤ì›Œë“œëŠ” ë¹ˆë„ìˆ˜ ê¸°ë°˜(keywords_freq) ë˜ëŠ” ì˜ë¯¸ ê¸°ë°˜(keywords_meaning) ì¤‘ ì„ íƒ ê°€ëŠ¥
                    # ì—¬ê¸°ì„œëŠ” ë¹ˆë„ìˆ˜ ê¸°ë°˜ ìƒìœ„ í‚¤ì›Œë“œë¥¼ ì‚¬ìš©
                    target_keywords_for_similar = keywords_freq 
                    displayed_similar_count = 0
                    for keyword_to_check in target_keywords_for_similar[:10]:
                        if displayed_similar_count >= 5: break
                        if keyword_to_check in model.wv:
                            similar_words = model.wv.most_similar(keyword_to_check, topn=5)
                            st.write(f"**'{keyword_to_check}'**ì™€ ìœ ì‚¬í•œ ë‹¨ì–´:")
                            st.write([f"{word} (ìœ ì‚¬ë„: {similarity:.2f})" for word, similarity in similar_words])
                            displayed_similar_count += 1
                    if displayed_similar_count == 0:
                        st.info("ì£¼ìš” í‚¤ì›Œë“œì— ëŒ€í•œ ìœ ì‚¬ ë‹¨ì–´ë¥¼ ëª¨ë¸ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤.")

                # --- 4. í‚¤ì›Œë“œì™€ ì—°ê´€ì„± ë†’ì€ ë¬¸ì¥ ì°¾ê¸° ---
                if model:
                    st.subheader("ğŸ“œ ì—°ê´€ì„± ë†’ì€ ë¬¸ì¥")
                    # ì—°ê´€ ë¬¸ì¥ ì°¾ê¸°ì˜ ëŒ€ìƒ í‚¤ì›Œë“œë„ ë¹ˆë„ìˆ˜ ê¸°ë°˜(keywords_freq) ë˜ëŠ” ì˜ë¯¸ ê¸°ë°˜(keywords_meaning) ì¤‘ ì„ íƒ
                    target_keywords_for_sentence = keywords_freq
                    # ... (ì´í•˜ ì—°ê´€ ë¬¸ì¥ ì°¾ê¸° ë¡œì§ì€ ì´ì „ê³¼ ë™ì¼, target_keywords_for_sentence ì‚¬ìš©) ...
                    num_top_sentences = 3
                    displayed_sentence_count = 0
                    for i in range(min(len(target_keywords_for_sentence), 10)):
                        if displayed_sentence_count >= 5: break
                        main_keyword = target_keywords_for_sentence[i]
                        if main_keyword not in model.wv: continue
                        sentence_similarities = []
                        for idx, sentence_nouns in enumerate(sentences_for_w2v):
                            if not sentence_nouns: continue
                            vectors = [model.wv[token] for token in sentence_nouns if token in model.wv]
                            if not vectors: continue
                            sentence_vector = np.mean(vectors, axis=0)
                            keyword_vector = model.wv[main_keyword]
                            similarity_score = cosine_similarity([sentence_vector], [keyword_vector])[0][0]
                            if idx < len(original_sentences_for_display):
                                sentence_similarities.append({
                                    'sentence': original_sentences_for_display[idx],
                                    'similarity': similarity_score
                                })
                        if sentence_similarities:
                            st.markdown(f"--- \n#### '{main_keyword}' ê´€ë ¨ ë¬¸ì¥:")
                            sorted_sentences = sorted(sentence_similarities, key=lambda x: x['similarity'], reverse=True)
                            for item in sorted_sentences[:num_top_sentences]:
                                st.markdown(f"> {item['sentence']} *(ìœ ì‚¬ë„: {item['similarity']:.3f})*")
                            displayed_sentence_count +=1
                    if displayed_sentence_count == 0:
                        st.info("ì£¼ìš” í‚¤ì›Œë“œì— ëŒ€í•œ ì—°ê´€ ë¬¸ì¥ì„ ì°¾ì„ ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤.")
else:
    if not uploaded_pdf_file and not raw_sentence_input_area.strip():
        st.info("ìƒê¸°ë¶€ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê±°ë‚˜, í…ìŠ¤íŠ¸ë¥¼ ì§ì ‘ ì…ë ¥í•´ì£¼ì„¸ìš”.")

# --- ì‚¬ì´ë“œë°” (ì´ì „ê³¼ ë™ì¼) ---
# ... (ì‚¬ì´ë“œë°” ì½”ë“œëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€) ...
st.sidebar.header("â„¹ï¸ ì‚¬ìš© ë°©ë²•")
st.sidebar.markdown("""
1.  **ìƒê¸°ë¶€ ë°ì´í„° ì…ë ¥:**
    * **PDF íŒŒì¼ ì—…ë¡œë“œ:** 'PDF íŒŒì¼ ì—…ë¡œë“œ' ì„¹ì…˜ì—ì„œ íŒŒì¼ì„ ì„ íƒí•©ë‹ˆë‹¤. (ê¶Œì¥)
    * **í…ìŠ¤íŠ¸ ì§ì ‘ ì…ë ¥:** PDFê°€ ì—†ì„ ê²½ìš°, ì•„ë˜ í…ìŠ¤íŠ¸ ì˜ì—­ì— ë‚´ìš©ì„ ë¶™ì—¬ë„£ìŠµë‹ˆë‹¤.
2.  **ë¶„ì„ ì‹œì‘:** 'ë¶„ì„ ì‹œì‘ âœ¨' ë²„íŠ¼ì„ í´ë¦­í•©ë‹ˆë‹¤. (ë°ì´í„°ê°€ ì…ë ¥ë˜ë©´ ë²„íŠ¼ì´ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.)
3.  **ê²°ê³¼ í™•ì¸:**
    * **ì£¼ìš” í‚¤ì›Œë“œ (ë¹ˆë„ìˆ˜ ê¸°ë°˜)**: ë‹¨ìˆœíˆ ìì£¼ ë“±ì¥í•˜ëŠ” ëª…ì‚¬ì…ë‹ˆë‹¤.
    * **ì£¼ìš” í‚¤ì›Œë“œ (ì˜ë¯¸ ê¸°ë°˜)**: ë¬¸ì„œ ì „ì²´ì˜ ì£¼ì œì™€ ê´€ë ¨ì„±ì´ ë†’ì€ ëª…ì‚¬ì…ë‹ˆë‹¤.
    * **ìœ ì‚¬ ë‹¨ì–´**, **ì—°ê´€ì„± ë†’ì€ ë¬¸ì¥**ì„ í™•ì¸í•©ë‹ˆë‹¤.

**íŒ:**
* PDF íŒŒì¼ì€ í…ìŠ¤íŠ¸ ê¸°ë°˜ì´ì–´ì•¼ ì •í™•í•œ ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. (ì´ë¯¸ì§€ ìŠ¤ìº” PDFëŠ” ì§€ì› X)
* ë¶ˆìš©ì–´ ëª©ë¡ì€ ì•± ì½”ë“œ ë‚´ì—ì„œ ì§ì ‘ ìˆ˜ì •í•˜ì—¬ ë¶„ì„ì˜ ì§ˆì„ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
""")
st.sidebar.header("âš™ï¸ ì„¤ì •ê°’ ì •ë³´")
st.sidebar.markdown(f"""
-   ì¶”ì¶œ ëª…ì‚¬ ìµœì†Œ ê¸¸ì´: `{MIN_NOUN_LEN}`
-   Word2Vec ìµœì†Œ ë‹¨ì–´ ë¹ˆë„: `{MIN_WORD_COUNT_FOR_W2V}`
""")


st.markdown("---") # êµ¬ë¶„ì„  ì¶”ê°€
st.header("ğŸ“ í”„ë¡œê·¸ë¨ í”¼ë“œë°±")
st.markdown("í”„ë¡œê·¸ë¨ ì‚¬ìš© ê²½í—˜ì— ëŒ€í•œ ì†Œì¤‘í•œ ì˜ê²¬ì„ ë‚¨ê²¨ì£¼ì„¸ìš”! ë²„ê·¸ ë¦¬í¬íŠ¸, ê°œì„  ì•„ì´ë””ì–´, ì¹­ì°¬ ëª¨ë‘ í™˜ì˜í•©ë‹ˆë‹¤. ğŸ˜Š")

# ì‚¬ìš©ì ì´ë¦„ ë˜ëŠ” ë‹‰ë„¤ì„ ì…ë ¥ (ì„ íƒ ì‚¬í•­) - ìƒí’ˆ ì¦ì •ìš©ìœ¼ë¡œ ìœ ì§€
user_info = st.text_input(
    "í•™ë²ˆ+ì´ë¦„ (ì„ íƒ ì‚¬í•­):",
    placeholder="ì¶”í›„ ìƒí’ˆ ì¦ì •ì— ì‚¬ìš©ë©ë‹ˆë‹¤. ê·¸ ì™¸ ì‚¬ìš©X",
    key="user_info_input_v3" # ì´ì „ keyì™€ ë‹¤ë¥´ê²Œ í•˜ê±°ë‚˜, ë™ì¼í•˜ê²Œ ì‚¬ìš©í•´ë„ ë¬´ë°© (ë‹¨, session_state ì´ˆê¸°í™” ì‹œ ì£¼ì˜)
)

# í”¼ë“œë°± ë‚´ìš© ì…ë ¥
feedback_text = st.text_area(
    "í”¼ë“œë°± ë‚´ìš©:",
    placeholder="ì—¬ê¸°ì— ìì„¸í•œ ë‚´ìš©ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.",
    height=150,
    key="feedback_text_area_v3" # ì´ì „ keyì™€ ë‹¤ë¥´ê²Œ í•˜ê±°ë‚˜, ë™ì¼í•˜ê²Œ ì‚¬ìš©í•´ë„ ë¬´ë°©
)

# ì œì¶œ ë²„íŠ¼
submit_button = st.button("í”¼ë“œë°± ì œì¶œí•˜ê¸°", key="feedback_submit_button_v3")

# --- í”¼ë“œë°± ì²˜ë¦¬ ë¡œì§ (GitHub Issue ìƒì„±) ---
if submit_button:
    if feedback_text.strip(): # í”¼ë“œë°± ë‚´ìš©ì´ ë¹„ì–´ìˆì§€ ì•Šë‹¤ë©´
        try:
            # Streamlit Secretsì—ì„œ í† í° ë° ì €ì¥ì†Œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            gh_token = st.secrets.get("GITHUB_TOKEN")
            repo_name = st.secrets.get("GITHUB_REPO")

            if not gh_token or not repo_name:
                st.error("GitHub í† í° ë˜ëŠ” ì €ì¥ì†Œ ì •ë³´ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì•± ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.")
            else:
                g = Github(gh_token)
                repo = g.get_repo(repo_name)

                # ì´ìŠˆ ì œëª© ë° ë³¸ë¬¸ êµ¬ì„± (í”¼ë“œë°± ìœ í˜• ì œê±°)
                submitter_id_for_title = user_info.strip() if user_info.strip() else "ìµëª… ì‚¬ìš©ì"
                # í”¼ë“œë°± ë‚´ìš©ì˜ ì¼ë¶€ë¥¼ ì œëª©ì— í¬í•¨ì‹œí‚¤ê±°ë‚˜, ë‹¨ìˆœíˆ "í”¼ë“œë°± ì œì¶œ" ë“±ìœ¼ë¡œ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                # ì—¬ê¸°ì„œëŠ” ì œì¶œì ì •ë³´ë§Œìœ¼ë¡œ ì œëª©ì„ êµ¬ì„±í•©ë‹ˆë‹¤.
                issue_title = f"í”¼ë“œë°± ì œì¶œ: {submitter_id_for_title}"

                issue_body = f"""
**ì œì¶œì ì •ë³´ (ìƒí’ˆ ì¦ì •ìš©, ì„ íƒ ì‚¬í•­):** {user_info.strip() if user_info.strip() else "ë¯¸ì…ë ¥"}
---
**ë‚´ìš©:**
{feedback_text}
"""
                # ì´ìŠˆ ìƒì„±
                created_issue = repo.create_issue(title=issue_title, body=issue_body)
                st.success("ì†Œì¤‘í•œ í”¼ë“œë°±ì´ ì„±ê³µì ìœ¼ë¡œ ì œì¶œë˜ì—ˆìŠµë‹ˆë‹¤! ê°ì‚¬í•©ë‹ˆë‹¤.")
                st.markdown(f"ì œì¶œëœ ë‚´ìš©ì€ [ì—¬ê¸°]({created_issue.html_url})ì—ì„œ (ê°œë°œìê°€) í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                st.info("í”¼ë“œë°± ë‚´ìš©ì€ GitHub ì €ì¥ì†Œì˜ 'Issues' íƒ­ì— ê¸°ë¡ë©ë‹ˆë‹¤.")

        except Exception as e:
            st.error(f"í”¼ë“œë°± ì œì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            st.error("ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”. ë¬¸ì œê°€ ì§€ì†ë˜ë©´ ì•± ê´€ë¦¬ìì—ê²Œ ì•Œë ¤ì£¼ì„¸ìš”.")
    else: # í”¼ë“œë°± ë‚´ìš©ì´ ë¹„ì–´ìˆë‹¤ë©´
        st.error("í”¼ë“œë°± ë‚´ìš©ì„ ì‘ì„±í•´ì£¼ì„¸ìš”! ğŸ˜…")

st.sidebar.markdown("---")
st.sidebar.caption("Made with Streamlit, KoNLPy, PyMuPDF & Word2Vec")

st.sidebar.markdown("---")
st.sidebar.caption("Made with Streamlit, KoNLPy & PyMuPDF")
