from collections import Counter
import streamlit as st
from gensim.models import Word2Vec
import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import re
from konlpy.tag import Okt
import fitz
#from github import Github
import requests
import json

# --- Okt í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” ---
okt = Okt()

# --- ë¶ˆìš©ì–´ ë° ê¸°ë³¸ ì„¤ì • (ì‚¬ìš©ìë‹˜ì˜ ìµœì‹  ë¶ˆìš©ì–´ ëª©ë¡ ì‚¬ìš©) ---
STOPWORDS = [
    'ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ë•Œ', 'ë“±', 'ë°', 'ë…„', 'ì›”', 'ì¼', 'ì¢€', 'ì¤‘', 'ìœ„í•´',
    'ê·¸ê²ƒ', 'ì´ê²ƒ', 'ì €ê²ƒ', 'ì—¬ê¸°', 'ì €ê¸°', 'ê±°ê¸°', 'ìì‹ ', 'ìì²´', 'ëŒ€í•œ', 'í†µí•´', 'ê´€ë ¨',
    'ì—¬ëŸ¬', 'ê°€ì§€', 'ë‹¤ë¥¸', 'ë¶€ë¶„', 'ê²½ìš°', 'ì •ë„', 'ì‚¬ì´', 'ë¬¸ì œ', 'ë‚´ìš©', 'ê²°ê³¼', 'ê³¼ì •',
    'ì‚¬ìš©', 'ìƒê°', 'ì§€ê¸ˆ', 'í˜„ì¬', 'ë‹¹ì‹œ', 'ë•Œë¬¸ì—', 'ë©´ì„œ', 'ë™ì•ˆ', 'ìœ„í•œ', 'ë”°ë¼',
    'ëŒ€í•´', 'í†µí•œ', 'ê´€ë ¨ëœ', 'ìˆìŒ', 'ì—†ìŒ', 'ê°™ìŒ', 'ì‚¬í•­', 'í™œë™', 'ëª¨ìŠµ', 'ë¶„ì•¼',
    'ëŠ¥ë ¥', 'ì—­ëŸ‰', 'ìì„¸', 'íƒœë„', 'ë…¸ë ¥', 'ë°”íƒ•', 'ì—­í• ', 'í•™ìŠµ', 'ì´í•´',
    'í•­ìƒ', 'ë§¤ìš°', 'ë‹¤ì†Œ', 'íŠ¹íˆ', 'ê°€ì¥', 'ë”ìš±', 'ì ê·¹ì ', 'êµ¬ì²´ì ', 'ë‹¤ì–‘í•œ', 'ê¾¸ì¤€íˆ',
    'ë›°ì–´ë‚¨', 'ìš°ìˆ˜í•¨', 'ë³´ì„', 'ë°œíœ˜í•¨', 'ì°¸ì—¬í•¨', 'íƒêµ¬í•¨', 'ë°œì „í•¨', 'í–¥ìƒë¨', 'í•¨ì–‘í•¨',
    'ë§Œë“¦', 'ë°œí‘œí•¨', 'ì œì‹œí•¨', 'ì œì¶œí•¨', 'ë°”', 'ì ', 'ì¸¡ë©´', 'ê³¼ì œ', 'ì¡°ì‚¬', 'ì£¼ì œ',
    'ìë£Œ', 'ë°œí‘œ', 'í† ë¡ ', 'ë³´ê³ ì„œ', 'íƒêµ¬', 'ì—°êµ¬', 'í”„ë¡œì íŠ¸', 'ì‹¤í—˜', 'ìˆ˜ì—…', 'ì‹œê°„',
    'ì´ìš©', 'ì°¸ì—¬',
    'ê³ ', 'í•œ', 'í„°', 'ì´í›„', 'ì´ì „', 'ë‚´', 'ì™¸', 'ì†',
    'ì—´ì‹¬íˆ',
    'í•˜ë‚˜', 'ë‘˜', 'ì…‹', 'ë„·', 'ë‹¤ì„¯', 'ì—¬ì„¯', 'ì¼ê³±', 'ì—¬ëŸ', 'ì•„í™‰', 'ì—´',
    'ì²«ì§¸', 'ë‘˜ì§¸', 'ì…‹ì§¸', 'ë‹¤ìŒ', 'ë¨¼ì €', 'ë¹„ë¡¯', 'ë¹„ë¡¯í•œ', 'ë“±ë“±', 'ê¸°íƒ€',
    'í™œìš©', 'ì‹¤ì‹œ', 'ì§„í–‰', 'ìˆ˜í–‰', 'ì œì‘', 'ê²½í—˜',
    'ê´€ì°°', 'ê¸°ë¡', 'ì •ë¦¬',
    'ê¸°ë°˜', 'í–¥ìƒ', 'ë°œì „', 'ì„±ì¥',
    'ìˆ˜ì¤€', 'ê´€ì‹¬', 'í¥ë¯¸', 'í˜¸ê¸°ì‹¬', 'ì§ˆë¬¸', 'ì œì•ˆ',
    'í•´ê²°', 'ë„ì›€', 'í˜‘ë ¥', 'ì†Œí†µ', 'ê´€ê³„', 'ì¤‘ì‹¬', 'ëŒ€ìƒ', 'ë°©ë²•', 'ì›ë¦¬', 'ê°œë…',
    'ì˜ë¯¸', 'ì¤‘ìš”ì„±', 'í•„ìš”ì„±', 'ê°€ì¹˜', 'ë‹¤ì–‘ì„±', 'ì°½ì˜ì„±', 'ì ê·¹ì„±', 'ì„±ì‹¤ì„±', 'ì±…ì„ê°',
    'ìê¸°ì£¼ë„', 'ëª¨ë²”', 'ë¦¬ë”ì‹­', 'íŒ”ë¡œìš°ì‹­', 'ê³µë™ì²´', 'ë°°ë ¤', 'ë‚˜ëˆ”', 'ë´‰ì‚¬',
    'êµê³¼', 'ê³¼ëª©', 'ë‹¨ì›', 'ì˜ì—­',
    'í•™ê¸°', 'í•™ë…„', 'í•™êµ', 'êµë‚´', 'êµì™¸',
    'ëŒ€íšŒ', 'í–‰ì‚¬', 'ìº í”„', 'ë™ì•„ë¦¬', 'ë¶€ì„œ', 'ì¡°ì§', 'ë‹¨ì²´', 'ê¸°ê´€', 'ì‹œì„¤',
    'í•™ìƒ', 'êµì‚¬', 'ì¹œêµ¬', 'ìš°ë¦¬', 'ëª¨ë‘ ', 'íŒ€',
    'ì‹œì‘', 'ë§ˆë¬´ë¦¬', 'ì™„ì„±',
    'ë“œëŸ¬ëƒ„', 'ê°–ì¶¤', 'ì§€ë‹˜', 'ì¸ì •ë¨', 'í™•ì¸ë¨', 'ê´€ì°°ë¨',
    'ìš°ìˆ˜', 'ë›°ì–´ë‚¨', 'íƒì›”', 'ë¯¸í¡', 'ë¶€ì¡±',
    'ê´€ë ¨í•˜ì—¬', 'ëŒ€í•˜ì—¬', 'ë°”íƒ•ìœ¼ë¡œ', 'ì¤‘ì‹¬ìœ¼ë¡œ', 'í†µí•˜ì—¬', 'ë¹„ì¶”ì–´', 'ì•ì„œ',
    'ê¸°ë¡í•¨', 'ê¸°ì¬í•¨', 'ì‘ì„±í•¨',
    'ë¨', 'í•¨', 'ë†’ìŒ', 'ë‚®ìŒ', 'ë§ìŒ', 'ì ìŒ',
    'ê¸°ëŒ€ë¨', 'ìš”ë§ë¨'
    'ìƒì›','ê³ ë“±í•™êµ','ìƒì›ê³ ë“±í•™êµ','ë²ˆí˜¸','í‘œí˜„','ì„¤ëª…','ì´ë¦„','ì„±í•¨'
]
MIN_NOUN_LEN = 2
MIN_WORD_COUNT_FOR_W2V = 1

# --- PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ í•¨ìˆ˜ (ì´ì „ê³¼ ë™ì¼) ---
def extract_text_from_pdf(uploaded_file):
    text = ""
    try:
        pdf_document = fitz.open(stream=uploaded_file.read(), filetype="pdf")
        for page_num in range(len(pdf_document)):
            page = pdf_document.load_page(page_num)
            text += page.get_text()
        pdf_document.close()
    except Exception as e:
        st.error(f"PDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None
    return text

# --- ëª…ì‚¬ ì¶”ì¶œ í•¨ìˆ˜ (ì´ì „ê³¼ ë™ì¼) ---
def extract_meaningful_nouns(text):
    text = re.sub(r"[^ê°€-í£ã„±-ã…ã…-ã…£a-zA-Z0-9\s.]+", "", str(text)).strip()
    text = re.sub(r"\s+", " ", text)
    if not text: return []
    nouns = okt.nouns(text)
    meaningful_nouns = []
    for noun in nouns:
        if (noun not in STOPWORDS and len(noun) >= MIN_NOUN_LEN and not noun.isnumeric()):
            meaningful_nouns.append(noun)
    return meaningful_nouns

# --- ë¹ˆë„ìˆ˜ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜ (ì´ì „ê³¼ ë™ì¼) ---
def get_keywords_from_nouns_by_freq(noun_list): # í•¨ìˆ˜ ì´ë¦„ ë³€ê²½
    if not noun_list: return [], []
    word_counts = Counter(noun_list)
    sorted_keywords_with_counts = word_counts.most_common()
    wordset = [item[0] for item in sorted_keywords_with_counts]
    wordsetcount = [item[1] for item in sorted_keywords_with_counts]
    return wordset, wordsetcount

# --- Streamlit UI (ì¼ë¶€ë§Œ í‘œì‹œ, í•µì‹¬ ë¡œì§ ìœ„ì£¼) ---
st.set_page_config(page_title="ìƒê¸°ë¶€ ë¶„ì„ê¸°", layout="wide")
st.title("ğŸ“ ìƒê¸°ë¶€ í‚¤ì›Œë“œ ë¶„ì„ ë° ì—°ê´€ ë¬¸ì¥ ì¶”ì²œ")
# ... (UI ìƒë‹¨ ë§ˆí¬ë‹¤ìš´, íŒŒì¼ ì—…ë¡œë“œ, í…ìŠ¤íŠ¸ ì…ë ¥ ë¶€ë¶„ì€ ì´ì „ê³¼ ê±°ì˜ ë™ì¼í•˜ê²Œ ìœ ì§€) ...
st.markdown("""
KoNLPy í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª…ì‚¬ ìœ„ì£¼ë¡œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ê³ ,
Word2Vec ëª¨ë¸ì„ í†µí•´ ìœ ì‚¬ ë‹¨ì–´ ë° ê´€ë ¨ ë†’ì€ ë¬¸ì¥ì„ ì°¾ì•„ì¤ë‹ˆë‹¤.
**PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê±°ë‚˜ í…ìŠ¤íŠ¸ë¥¼ ì§ì ‘ ì…ë ¥í•˜ì—¬ ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.**
""")
st.subheader("1. ë¶„ì„í•  ìƒê¸°ë¶€ ë°ì´í„° ì…ë ¥")
st.markdown("[ì¹´ì¹´ì˜¤í†¡ì—ì„œ ìƒê¸°ë¶€ pdf ë‹¤ìš´ë°›ëŠ”ë²•(ê¶Œì¥!!)](https://blog.naver.com/needtime0514/223256443411)", unsafe_allow_html=True)
st.markdown("[ì •ë¶€24ì—ì„œ ìƒê¸°ë¶€ pdf ë‹¤ìš´ë°›ëŠ”ë²•](https://blog.naver.com/leeyju4/223208661500)", unsafe_allow_html=True)

uploaded_pdf_file = st.file_uploader("ìƒê¸°ë¶€ PDF íŒŒì¼ ì—…ë¡œë“œ (PDF ì—…ë¡œë“œ ì‹œ ì•„ë˜ í…ìŠ¤íŠ¸ ì…ë ¥ ë‚´ìš©ì€ ë¬´ì‹œë©ë‹ˆë‹¤):", type="pdf")
raw_sentence_input_area = st.text_area("ë˜ëŠ”, ìƒê¸°ë¶€ ë‚´ìš©ì„ ì—¬ê¸°ì— ì§ì ‘ ë¶™ì—¬ë„£ìœ¼ì„¸ìš”:", height=200, placeholder="PDFë¥¼ ì—…ë¡œë“œí•˜ì§€ ì•Šì„ ê²½ìš° ì—¬ê¸°ì— í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”...")

raw_sentence_input = None
if uploaded_pdf_file is not None:
    with st.spinner("PDF íŒŒì¼ì„ ì½ê³  ë¶„ì„ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤..."):
        extracted_text_from_pdf = extract_text_from_pdf(uploaded_pdf_file)
        if extracted_text_from_pdf:
            raw_sentence_input = extracted_text_from_pdf
            st.success("PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤!")
        else:
            st.error("PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. íŒŒì¼ì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•˜ê±°ë‚˜, ì•„ë˜ í…ìŠ¤íŠ¸ ì˜ì—­ì— ì§ì ‘ ì…ë ¥í•´ì£¼ì„¸ìš”.")
elif raw_sentence_input_area.strip():
    raw_sentence_input = raw_sentence_input_area
else:
    pass

# ... (ì´ì „ ì½”ë“œë“¤ì€ ë™ì¼í•˜ê²Œ ìœ ì§€) ...

if raw_sentence_input and raw_sentence_input.strip():
    if st.button("ë¶„ì„ ì‹œì‘ âœ¨"):
        with st.spinner('í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤... (KoNLPy/Word2Vec ì²« ì‹¤í–‰ ì‹œ ì‹œê°„ì´ ë” ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤) â³'):
            all_document_nouns = extract_meaningful_nouns(raw_sentence_input)

            if not all_document_nouns:
                st.error("ë¶„ì„í•  ì˜ë¯¸ ìˆëŠ” ëª…ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤. ì…ë ¥ ë‚´ìš©ì„ í™•ì¸í•˜ê±°ë‚˜ ë¶ˆìš©ì–´ ì„¤ì •ì„ ì ê²€í•´ì£¼ì„¸ìš”.")
            else:
                # --- 1. ë¹ˆë„ìˆ˜ ê¸°ë°˜ í‚¤ì›Œë“œ í‘œì‹œ (ê¸°ì¡´ ë°©ì‹) ---
                st.subheader("ğŸ”‘ ì£¼ìš” í‚¤ì›Œë“œ (ë‹¨ìˆœ ë¹ˆë„ìˆ˜ ê¸°ë°˜)")
                keywords_freq_raw, keyword_counts_freq_raw = get_keywords_from_nouns_by_freq(all_document_nouns)
                
                # "ìƒì›" ë° ê¸°íƒ€ ëª…ì‹œì ìœ¼ë¡œ ì œê±°í•˜ê³  ì‹¶ì€ ë‹¨ì–´ë“¤ ë¦¬ìŠ¤íŠ¸
                explicit_remove_list = ["ìƒì›"] # í•„ìš”ì— ë”°ë¼ ì¶”ê°€

                # ë¹ˆë„ìˆ˜ ê¸°ë°˜ í‚¤ì›Œë“œì—ì„œ "ìƒì›" ë“± ì œê±°
                keywords_freq_filtered = []
                keyword_counts_freq_filtered = []
                for kw, count in zip(keywords_freq_raw, keyword_counts_freq_raw):
                    if kw not in explicit_remove_list:
                        keywords_freq_filtered.append(kw)
                        keyword_counts_freq_filtered.append(count)
                
                if not keywords_freq_filtered:
                    st.warning("ë¹ˆë„ìˆ˜ ê¸°ë°˜ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤ (í•„í„°ë§ í›„).")
                else:
                    keyword_df_freq = pd.DataFrame({'í‚¤ì›Œë“œ': keywords_freq_filtered, 'ë¹ˆë„ìˆ˜': keyword_counts_freq_filtered})
                    st.dataframe(keyword_df_freq.head(10))

                # --- Word2Vec ëª¨ë¸ í•™ìŠµ (ì´ì „ê³¼ ë™ì¼) ---
                # ... (sentences_for_w2v, model í•™ìŠµ ë¡œì§ì€ ê·¸ëŒ€ë¡œ) ...
                raw_sentences = re.split(r'(?<=[.?!])\s+', raw_sentence_input.strip())
                sentences_for_w2v = []
                original_sentences_for_display = []
                for sentence_text in raw_sentences:
                    sentence_text_cleaned = sentence_text.strip()
                    if sentence_text_cleaned:
                        # Word2Vec í•™ìŠµ ë°ì´í„°ì—ëŠ” "ìƒì›"ì´ ë¶ˆìš©ì–´ ì²˜ë¦¬ë˜ì–´ ë¹ ì§€ëŠ” ê²ƒì´ ì´ìƒì ì´ì§€ë§Œ,
                        # ë§Œì•½ extract_meaningful_nounsì—ì„œ ì—¬ì „íˆ ë¬¸ì œê°€ ìˆë‹¤ë©´ ì—¬ê¸°ì—ë„ ì˜í–¥.
                        # í•˜ì§€ë§Œ extract_meaningful_nounsì˜ STOPWORDSëŠ” ê³„ì† ìœ ì§€/ê°œì„ í•´ì•¼ í•¨.
                        sentence_nouns = extract_meaningful_nouns(sentence_text_cleaned)
                        if sentence_nouns:
                            sentences_for_w2v.append(sentence_nouns)
                            original_sentences_for_display.append(sentence_text_cleaned)
                
                model = None # ì´ˆê¸°í™”
                if not sentences_for_w2v or len(sentences_for_w2v) < 1:
                    st.error("Word2Vec ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ ë¬¸ì¥(ëª…ì‚¬ ê¸°ë°˜) ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
                else:
                    try:
                        model = Word2Vec(sentences_for_w2v, vector_size=100, window=5, min_count=MIN_WORD_COUNT_FOR_W2V, workers=4, sg=1)
                        st.success("Word2Vec ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
                    except Exception as e:
                        st.error(f"Word2Vec ëª¨ë¸ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


                # --- 2. ì˜ë¯¸ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ (ë¬¸ì„œ ë²¡í„°ì™€ ìœ ì‚¬ë„) ---
                if model:
                    st.subheader("ğŸŒŸ ì£¼ìš” í‚¤ì›Œë“œ (ë¬¸ì„œ ì „ì²´ ì˜ë¯¸ ê¸°ë°˜)")
                    doc_vector_sum = np.zeros(model.vector_size)
                    word_count_for_doc_vector = 0
                    valid_nouns_for_doc_vector = [noun for noun in all_document_nouns if noun in model.wv and noun not in explicit_remove_list] # ì—¬ê¸°ì„œë„ ì œê±°
                    
                    if not valid_nouns_for_doc_vector:
                        st.warning("ë¬¸ì„œ ëŒ€í‘œ ë²¡í„°ë¥¼ ê³„ì‚°í•˜ê±°ë‚˜ ì˜ë¯¸ ê¸°ë°˜ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•  ë‹¨ì–´ê°€ ëª¨ë¸ì— ì—†ìŠµë‹ˆë‹¤ (í•„í„°ë§ í›„).")
                    else:
                        for word in valid_nouns_for_doc_vector:
                            doc_vector_sum += model.wv[word]
                            word_count_for_doc_vector += 1
                        
                        if word_count_for_doc_vector > 0:
                            document_vector = doc_vector_sum / word_count_for_doc_vector
                            candidate_keywords = sorted(list(set(valid_nouns_for_doc_vector))) # ì´ë¯¸ explicit_remove_list ì œì™¸ë¨
                            
                            keyword_similarities_to_doc = []
                            for keyword_candidate in candidate_keywords: # ì´ë¯¸ explicit_remove_list ì œì™¸ë¨
                                try:
                                    similarity = cosine_similarity([model.wv[keyword_candidate]], [document_vector])[0][0]
                                    keyword_similarities_to_doc.append((keyword_candidate, similarity))
                                except KeyError:
                                    continue 
                            
                            if keyword_similarities_to_doc:
                                sorted_keywords_by_meaning_raw = sorted(keyword_similarities_to_doc, key=lambda item: item[1], reverse=True)
                                
                                # ì˜ë¯¸ ê¸°ë°˜ í‚¤ì›Œë“œì—ì„œ "ìƒì›" ë“± ì œê±° (ì´ë¯¸ candidate_keywordsì—ì„œ ê³ ë ¤í–ˆì§€ë§Œ, í•œë²ˆ ë” í™•ì¸ ê°€ëŠ¥)
                                keywords_meaning_filtered = []
                                keyword_scores_meaning_filtered = []
                                for kw, score in sorted_keywords_by_meaning_raw:
                                    if kw not in explicit_remove_list: # ì´ì¤‘ ì²´í¬ ë˜ëŠ” ì—¬ê¸°ì„œë§Œ ì²˜ë¦¬
                                        keywords_meaning_filtered.append(kw)
                                        keyword_scores_meaning_filtered.append(score)

                                if not keywords_meaning_filtered:
                                    st.warning("ì˜ë¯¸ ê¸°ë°˜ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤ (í•„í„°ë§ í›„).")
                                else:
                                    keyword_df_meaning = pd.DataFrame({'í‚¤ì›Œë“œ': keywords_meaning_filtered, 'ë¬¸ì„œ ëŒ€í‘œ ë²¡í„°ì™€ì˜ ìœ ì‚¬ë„': keyword_scores_meaning_filtered})
                                    st.dataframe(keyword_df_meaning.head(15))
                            else:
                                st.warning("ì˜ë¯¸ ê¸°ë°˜ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                        else:
                             st.warning("ë¬¸ì„œ ëŒ€í‘œ ë²¡í„° ê³„ì‚°ì— ì‚¬ìš©ë  ìœ íš¨í•œ ë‹¨ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    st.warning("Word2Vec ëª¨ë¸ì´ ì—†ì–´ ì˜ë¯¸ ê¸°ë°˜ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

                # --- 3. í‚¤ì›Œë“œì™€ ì—°ê´€ì„± ë†’ì€ ë¬¸ì¥ ì°¾ê¸° ---
                if model:
                    st.subheader("ğŸ“œ ì—°ê´€ì„± ë†’ì€ ë¬¸ì¥")
                    # ì—°ê´€ ë¬¸ì¥ ì°¾ê¸° ëŒ€ìƒë„ í•„í„°ë§ëœ í‚¤ì›Œë“œ ì‚¬ìš©
                    target_keywords_for_sentence = keywords_freq_filtered # í•„í„°ë§ëœ ë¹ˆë„ìˆ˜ ê¸°ë°˜ í‚¤ì›Œë“œ ì‚¬ìš©
                    # ... (ì´í•˜ ë¡œì§ ë™ì¼, target_keywords_for_sentence ì‚¬ìš©) ...
                    displayed_sentence_count = 0
                    for i in range(min(len(target_keywords_for_sentence), 10)):
                        if displayed_sentence_count >= 10: break
                        main_keyword = target_keywords_for_sentence[i]
                        if main_keyword not in model.wv: continue
                        sentence_similarities = []
                        for idx, sentence_nouns in enumerate(sentences_for_w2v):
                            if not sentence_nouns: continue
                            vectors = [model.wv[token] for token in sentence_nouns if token in model.wv]
                            if not vectors: continue
                            sentence_vector = np.mean(vectors, axis=0)
                            keyword_vector = model.wv[main_keyword]
                            similarity_score = cosine_similarity([sentence_vector], [keyword_vector])[0][0]
                            if idx < len(original_sentences_for_display):
                                sentence_similarities.append({
                                    'sentence': original_sentences_for_display[idx],
                                    'similarity': similarity_score
                                })
                        if sentence_similarities:
                            st.markdown(f"--- \n#### '{main_keyword}' ê´€ë ¨ ë¬¸ì¥:")
                            sorted_sentences = sorted(sentence_similarities, key=lambda x: x['similarity'], reverse=True)
                            num_top_sentences = 3
                            for item in sorted_sentences[:num_top_sentences]:
                                st.markdown(f"> {item['sentence']} *(ìœ ì‚¬ë„: {item['similarity']:.3f})*")
                            displayed_sentence_count +=1
                    if displayed_sentence_count == 0:
                        st.info("ì£¼ìš” í‚¤ì›Œë“œì— ëŒ€í•œ ì—°ê´€ ë¬¸ì¥ì„ ì°¾ì„ ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤.")


# --- ì‚¬ì´ë“œë°” (ì´ì „ê³¼ ë™ì¼) ---
# ... (ì‚¬ì´ë“œë°” ì½”ë“œëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€) ...
st.sidebar.header("â„¹ï¸ ì‚¬ìš© ë°©ë²•")
st.sidebar.markdown("""
1.  **ìƒê¸°ë¶€ ë°ì´í„° ì…ë ¥:**
    * **PDF íŒŒì¼ ì—…ë¡œë“œ:** 'PDF íŒŒì¼ ì—…ë¡œë“œ' ì„¹ì…˜ì—ì„œ íŒŒì¼ì„ ì„ íƒí•©ë‹ˆë‹¤. (ê¶Œì¥)
    * **í…ìŠ¤íŠ¸ ì§ì ‘ ì…ë ¥:** PDFê°€ ì—†ì„ ê²½ìš°, ì•„ë˜ í…ìŠ¤íŠ¸ ì˜ì—­ì— ë‚´ìš©ì„ ë¶™ì—¬ë„£ìŠµë‹ˆë‹¤.
2.  **ë¶„ì„ ì‹œì‘:** 'ë¶„ì„ ì‹œì‘ âœ¨' ë²„íŠ¼ì„ í´ë¦­í•©ë‹ˆë‹¤. (ë°ì´í„°ê°€ ì…ë ¥ë˜ë©´ ë²„íŠ¼ì´ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.)
3.  **ê²°ê³¼ í™•ì¸:**
    * **ì£¼ìš” í‚¤ì›Œë“œ (ë¹ˆë„ìˆ˜ ê¸°ë°˜)**: ë‹¨ìˆœíˆ ìì£¼ ë“±ì¥í•˜ëŠ” ëª…ì‚¬ì…ë‹ˆë‹¤.
    * **ì£¼ìš” í‚¤ì›Œë“œ (ì˜ë¯¸ ê¸°ë°˜)**: ë¬¸ì„œ ì „ì²´ì˜ ì£¼ì œì™€ ê´€ë ¨ì„±ì´ ë†’ì€ ëª…ì‚¬ì…ë‹ˆë‹¤.
    * **ìœ ì‚¬ ë‹¨ì–´**, **ì—°ê´€ì„± ë†’ì€ ë¬¸ì¥**ì„ í™•ì¸í•©ë‹ˆë‹¤.

**íŒ:**
* PDF íŒŒì¼ì€ í…ìŠ¤íŠ¸ ê¸°ë°˜ì´ì–´ì•¼ ì •í™•í•œ ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. (ì´ë¯¸ì§€ ìŠ¤ìº” PDFëŠ” ì§€ì› X)
* ë¶ˆìš©ì–´ ëª©ë¡ì€ ì•± ì½”ë“œ ë‚´ì—ì„œ ì§ì ‘ ìˆ˜ì •í•˜ì—¬ ë¶„ì„ì˜ ì§ˆì„ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
""")
st.sidebar.header("âš™ï¸ ì„¤ì •ê°’ ì •ë³´")
st.sidebar.markdown(f"""
-   ì¶”ì¶œ ëª…ì‚¬ ìµœì†Œ ê¸¸ì´: `{MIN_NOUN_LEN}`
-   Word2Vec ìµœì†Œ ë‹¨ì–´ ë¹ˆë„: `{MIN_WORD_COUNT_FOR_W2V}`
""")


st.markdown("---") # êµ¬ë¶„ì„  ì¶”ê°€
st.header("ğŸ“ í”„ë¡œê·¸ë¨ í”¼ë“œë°±")
st.markdown("ìƒí’ˆì„ ë°›ìœ¼ë ¤ë©´ êµ¬ê¸€ í¼ ë§í¬ë¥¼ í†µí•´ í”¼ë“œë°±ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”!")
st.markdown("[êµ¬ê¸€ í¼ ë§í¬](https://docs.google.com/forms/d/e/1FAIpQLSdqbJDR3ASS1IXqSh2dyo15xrl08sefT9N3-p7bJ1XzyWhvew/viewform?usp=header)", unsafe_allow_html=True)


st.sidebar.markdown("---")
st.sidebar.caption("Made with Streamlit, KoNLPy, PyMuPDF & Word2Vec")
